{
  
    
        "post0": {
            "title": "Predicting water solubility - Part II?",
            "content": "Requirements . rdkit &gt;= 2020.09.1 | pandas &gt;= 1.1.3 | seaborn | matplotlib | fastcore (!conda install fastcore) | . In this tutorial we&#39;ll use a dataset compiled by Sorkun et al (2019) from multiple projects to predict water solubility. You can download the original dataset from here. . At the end of the notebook I added a class to process the original dataset in order to remove salts, mixtures, neutralize charges and generate canonical SMILES. I highly recommend checking each structure before modeling. . In this notebook we&#39;ll cover three main topics: . Featurization of molecules | What&#39;s the correlation between features? | Feature selection methods | . Background . Drug solubility is a critical factor in drug development. If a drug is not soluble enough or doesn&#39;t dissolve readily its intestinal absoption will be compromised, leading to low concentration in the blood circulation and reduced (or none) biological activity. . Crystal formation of low soluble drugs may also lead to toxicity. In practical terms, poor solubility is one of the factors that lead to fail in drug discovery projects. Therefore, medicinal chemists work hard to design molecules tha have the intended bioactivity and that can display the desired effect in vivo. . Despite being conceptually easy to understand solubility, its estimation isn&#39;t easy. That&#39;s because the intrinsic solubility of a molecule depends on many factors, including its size, shape, the ability to make intermolecular interactions and crystal packing. . Import modules . %reload_ext autoreload %autoreload 2 %matplotlib inline from IPython.display import Image import matplotlib.pyplot as plt from matplotlib.ticker import FormatStrFormatter import seaborn as sns import pandas as pd import numpy as np from sklearn.preprocessing import Normalizer, normalize, RobustScaler from sklearn.ensemble import RandomForestRegressor from xgboost import XGBRegressor from sklearn.metrics import make_scorer, mean_squared_error from sklearn.feature_selection import mutual_info_regression, RFECV,SelectFromModel,RFE,VarianceThreshold from functools import partial from pathlib import Path from joblib import load, dump from scipy import stats from scipy.stats import norm from statsmodels.graphics.gofplots import qqplot from rdkit.Chem import Draw from rdkit.Chem import MolFromSmiles, MolToSmiles from descriptastorus.descriptors.DescriptorGenerator import MakeGenerator . . WARNING:root:No normalization for BCUT2D_MWHI WARNING:root:No normalization for BCUT2D_MWLOW WARNING:root:No normalization for BCUT2D_CHGHI WARNING:root:No normalization for BCUT2D_CHGLO WARNING:root:No normalization for BCUT2D_LOGPHI WARNING:root:No normalization for BCUT2D_LOGPLOW WARNING:root:No normalization for BCUT2D_MRHI WARNING:root:No normalization for BCUT2D_MRLOW . np.random.seed(5) . sns.set(rc={&#39;figure.figsize&#39;: (16, 16)}) sns.set_style(&#39;whitegrid&#39;) sns.set_context(&#39;paper&#39;,font_scale=1.5) . Load Data . Let&#39;s load the dataset without outliers from the last post. . data = pd.read_csv(&#39;../_data/water_solubility_nooutliers.csv&#39;) . data.head() . ID Name InChI InChIKey SMILES Solubility SD Ocurrences Group MolWt ... NumAromaticRings NumSaturatedRings NumAliphaticRings RingCount TPSA LabuteASA BalabanJ BertzCT processed_smiles class . 0 A-3 | N,N,N-trimethyloctadecan-1-aminium bromide | InChI=1S/C21H46N.BrH/c1-5-6-7-8-9-10-11-12-13-... | SZEMGTQCPRNXEG-UHFFFAOYSA-M | [Br-].CCCCCCCCCCCCCCCCCC[N+](C)(C)C | -3.616127 | 0.0 | 1 | G1 | 392.510 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.00 | 158.520601 | 0.000000 | 210.377334 | CCCCCCCCCCCCCCCCCC[N+](C)(C)C | slightly soluble | . 1 A-4 | Benzo[cd]indol-2(1H)-one | InChI=1S/C11H7NO/c13-11-8-5-1-3-7-4-2-6-9(12-1... | GPYLCFQEKPUWLD-UHFFFAOYSA-N | O=C1Nc2cccc3cccc1c23 | -3.254767 | 0.0 | 1 | G1 | 169.183 | ... | 2.0 | 0.0 | 1.0 | 3.0 | 29.10 | 75.183563 | 2.582996 | 511.229248 | O=C1Nc2cccc3cccc1c23 | slightly soluble | . 2 A-5 | 4-chlorobenzaldehyde | InChI=1S/C7H5ClO/c8-7-3-1-6(5-9)2-4-7/h1-5H | AVPYQKSLYISFPO-UHFFFAOYSA-N | Clc1ccc(C=O)cc1 | -2.177078 | 0.0 | 1 | G1 | 140.569 | ... | 1.0 | 0.0 | 0.0 | 1.0 | 17.07 | 58.261134 | 3.009782 | 202.661065 | O=Cc1ccc(Cl)cc1 | slightly soluble | . 3 A-10 | vinyltoluene | InChI=1S/C9H10/c1-3-9-6-4-5-8(2)7-9/h3-7H,1H2,2H3 | JZHGRUMIRATHIU-UHFFFAOYSA-N | Cc1cccc(C=C)c1 | -3.123150 | 0.0 | 1 | G1 | 118.179 | ... | 1.0 | 0.0 | 0.0 | 1.0 | 0.00 | 55.836626 | 3.070761 | 211.033225 | C=Cc1cccc(C)c1 | slightly soluble | . 4 A-11 | 3-(3-ethylcyclopentyl)propanoic acid | InChI=1S/C10H18O2/c1-2-8-3-4-9(7-8)5-6-10(11)1... | WVRFSLWCFASCIS-UHFFFAOYSA-N | CCC1CCC(CCC(O)=O)C1 | -3.286116 | 0.0 | 1 | G1 | 170.252 | ... | 0.0 | 1.0 | 1.0 | 1.0 | 37.30 | 73.973655 | 2.145839 | 153.917569 | CCC1CCC(CCC(=O)O)C1 | slightly soluble | . 5 rows × 28 columns . . Featurization . In this dataset 17 features were already calculated for each molecule. These features include a range of physichochemical properties (e.g. MolWt, MolLogP and MolMR), atomic counts (e.g. HeavyAtomCount, NumHDonors and Acceptors) and more abstract topological descriptors (e.g. BalabanJ, BertzCT). . descriptors = [&#39;MolWt&#39;, &#39;MolLogP&#39;, &#39;MolMR&#39;, &#39;HeavyAtomCount&#39;, &#39;NumHAcceptors&#39;, &#39;NumHDonors&#39;, &#39;NumHeteroatoms&#39;, &#39;NumRotatableBonds&#39;, &#39;NumValenceElectrons&#39;, &#39;NumAromaticRings&#39;, &#39;NumSaturatedRings&#39;, &#39;NumAliphaticRings&#39;, &#39;RingCount&#39;, &#39;TPSA&#39;, &#39;LabuteASA&#39;, &#39;BalabanJ&#39;, &#39;BertzCT&#39;] var = [&#39;Solubility&#39;]+ descriptors . print(len(descriptors)) . 17 . We can check the correlation between each feature and solubility using seaborn&#39;s regplot: . nr_rows = 6 nr_cols = 3 target = &#39;Solubility&#39; . Most features have a negative correlation with solubility. In fact, the available features reflect more or less the size/complexity of the molecule (e.g. MolWt, RingCount, MolMR) and it&#39;s capacity to make inter and intramolecular interactions (e.g., NumValenceElectrons, NumHAcceptors, NumHDonors, MolLogP), which are known to influence water solubility. For instance, bigger molecules tend to be less soluble in water (high MolMW and HeavyAtomCount); the same applies for very liphophilic (high MolLogP) molecules. On the other hand, more polar molecules, capable of making hydrogen bonds with water, are usually more soluble. . Based on this preliminary analysis we can start thinking which features to include when training a regression model. For instance, molecular weight and liphophilicity are the features most correlated with the target variable. These features have a strong influence in solubility as demonstrated experimentally and by theorical calculations. As the molecular size and liphophilicity grows, its much harder to dissolve a molecule in water because the solute needs to disrupt a great number interactions within solvent molecules and force its way into bulk water, which demands a great amount of energy. The influence of lipophilicity is so well known that its part of many predictive models, such as the famous ESOL (estimated solubility), which estimates water solubility based only on the molecular structure. . Back in the day (2003!) the author of ESOL didn&#39;t even use the robust machine learning algorithms we have today to derive the following equation: . $$logS_{w} = 0.16 - 0.63logP - 0.0062MolWt + 0.066Rot - -0.74AP$$ . where $logS_{w}$ is the log of the water solubility, $logP$ is the lipophilicity, $MolWt$ is the molecular weight, $Rot$ is the number of rotatable bonds (e.g. single bonds) and $AP$ is the proportion of heavy atoms that are part of aromatic rings. . Understanding the features . Let&#39;s take a look at what those features mean. . 1) LogP . The logP is the partition coefficient given by: . $$logP = frac{C_{n-octanol}}{C_{water}}$$ . where $C_{n-octanol}$ and $C_{water}$ are the solute concentration in n-octanol and water, respectively. Thus, higher logP means that a smaller concentration of the molecule is available in the water phase of a system, which makes it more lipophilic or hydrophobic. . 2) Molecular weight . Molecular weight (MolWt) is simply the total mass of a mole) of a compound. The MolWt can be calculated by the summing the mass contribution of each atom in a molecule. For example, the molar weight of water is: . $$H_{2}O = 2H * (1.01 g/mol) + 1O * (16 g/mol) = 18.01 g/mol$$ . 3) Molar refractivity . Molar refractivity (MolMR) is the refractivity of a mole of a substance and is a measure of polarizability. It&#39;s given by the formula: . $$MolMR = frac{n^{2}-1}{n^{2}+2}* frac{MolWt}{d}$$ . where $MolWt$ is the molar weight, $d$ is the density and $n$ is the refractivity index. The right hand side of the equation ($ frac{MolWt}{d}$) is the volume. Thus, the molar refractivity encodes the molecular volume and is a way to estimate the steric bulk. . 4) Topological polar surface area (TPSA) and LabuteASA . TPSA: The polar surface area is the sum of the surfaces of polar atoms in a molecule, such as oxygen and nitrogen, and sometimes sulphur and its hydrogen. The total polar surface area is a useful feature to encode both the polarity and the hydrogen bonding capacity of a molecule. | . LabuteASA: the accessible surface area (ASA) is the area of a molecule that is accessible to the solvent (e.g. water). The ASA is calculated by summing the surface area of each atom in a molecule. We can also think about ASA as the area a water molecule can touch as we roll it on the surface of the solute. | . . Figure 1. Representation of the polar surface area, showing the contributions of each polar atom in the molecule. . . Figure 2. Representation of the polar surface area, showing the contributions of each polar atom in the molecule. . 5) Atomic counts . Atomics counts represent how many times an atom or group appears in a molecule. Its possible to count specific substructures (e.g. number of aromatic rings), atoms and atoms types, hydrogen bond donors and acceptors etc. . 6) Topological descriptors . Topological descriptors in this dataset include BalabanJ and BertzCT and represent more abstract definition of a molecule. For the BalabanJ, the feature value is calculated from the distance matrix, a n x n matrix where n is the number of atoms and the entries represent the distances between each atom in the molecular graph. The BalabanJ descriptor can thus capture topological information such as branching, distance between substructures and the molecular size. The BertzCT descriptor encodes the molecular complexity by taking into account bond connectivity and atom types. . Now let&#39;s get to work! . In depth analysis of feature correlation . First, let&#39;s calculate the Pearson&#39;s R coefficient for each pair of features and visualize the results using a heatmap. . The largest diagonal corresponds to self correlation for each feature (e.g. MolWt x MolWt), and is always 1. The most interesting part are the other values, showing the correlation between each pair of features and target variable. . As we can see from the heatmap, some features have a high negative linear correlation with solubility, such as MolWt (-0.61), LabuteASA (-0.62), MolLogP (-0.78) and MolMR (-0.64), HeavyAtomCount (-0.57). . We can also investigate the correlations between features. For the BalabanJ descriptor, the highest correlation was with RingCount (-0.64). For BertzCT, the highest correlations were with features that encode the molecular complexity/size, such as molecular weight, molar refractivity and number of heavy atoms, which is consistent with its definition. . corr = data[var].corr(&#39;pearson&#39;) . sns.heatmap(corr, annot=True) . &lt;AxesSubplot:&gt; . . We can also see that the number of hydrogen bond donors have a weak positive correlation with solubility, while the number of H-bond acceptors have a very weak negative correlation. This is interesting because it shows us that increasing the H-bond potential might not be the best strategy to increase solubility. . In general, increasing H-bond donors and acceptors also increases solubility because it alters the capacity to make H-bonds with water molecules. However, if the molecule can both donate and accept H-bonds, a decrease in solubility might happen due to intermolecular interactions between molecules of the same kind, leading to reduced ability to be solvated by water molecules. The intuition is that a molecule needs to make better interactions with water than with others of its kind, otherwise it will be poorly solvated. . To summarize this initial analysis, the heatmap give us an idea of what types of features to include in a model. If we were to cherry pick, features like MolLogP, MolMR and LabuteASA would probably be selected. In addition, we must avoid highly correlated features (i.e. that encode the same kind of information) because that would only make our model more complex without any benefit in performance. . But no need to rush! We&#39;ll systematically check each feature for better correlations with the target variable and with each other. Remember that our goal is to develop a model with high predictive power, and to do that we need to select the right set of features. . Training a baseline model . Before we dive into feature selection, let&#39;s try a baseline model and see how it performs when training using all features. For this task, we will use the Random Forest algorithm. First, split the data into training and testing sets. . from sklearn.model_selection import train_test_split . trainset, testset = train_test_split(data, test_size = 0.30, random_state=42) . Since our features are in different scales, let&#39;s normalize them to have zero mean and unit std. This is an important step so that one feature with large std doesn&#39;t dominate the others during training, which could lead to overfitting. . Random forest is robust to the scale of features so this step isn&#39;t necessary in this case. We will normalize anyway because it&#39;s best practice, especially in models that depends on coefficients and intercepts (e.g. linear regression). . xtrain, xtest = trainset[descriptors].values, testset[descriptors].values ytrain, ytest = trainset[&#39;Solubility&#39;].values, testset[&#39;Solubility&#39;].values . We will do the preprocessing in a more compact way by using sklearn pipeline, which will contain the preprocessing steps and a fit call to the training algorithm. . from sklearn.pipeline import Pipeline . pipe = Pipeline(steps=[ (&#39;scaler&#39;,RobustScaler()), (&#39;estimator&#39;, RandomForestRegressor(n_estimators=2000, n_jobs=-1)) ] ) . We will validate our model using 5-fold cross-validation. . from sklearn.model_selection import cross_val_score . metric = make_scorer(mean_squared_error, squared=False) # RMSE cross_score = cross_val_score(estimator=pipe, X=xtrain,scoring=metric, y=ytrain, cv=5, n_jobs=-1) . print(f&#39;Mean 5-fold RMSE = {cross_score.mean():.4f}&#39;) . Mean 5-fold RMSE = 0.9983 . Now the test set: . pipe.fit(xtrain,ytrain) . Pipeline(steps=[(&#39;scaler&#39;, RobustScaler()), (&#39;estimator&#39;, RandomForestRegressor(n_estimators=2000, n_jobs=-1))]) . def get_preds(estimator, x): preds = estimator.predict(x) return preds . preds = pipe.predict(xtest) . print(f&#39;RMSE test set = {mean_squared_error(preds, ytest, squared=False):.3f}&#39;) . RMSE test set = 0.963 . Not bad! This is our baseline. Let&#39;s see if we can beat it or simplify our data a little bit without compromising performance. . The perils of non-informative features . What happens if we only have non-informative features in the dataset? What performance could we expect? Intuitively, one would say RMSE &gt;&gt; 0. Let&#39;s test that by randomizing the rows of the target variable vector, which will make every feature non-informative. . yrandom = np.random.choice(ytrain,ytrain.shape[0]) . yrandom.shape,ytrain.shape . ((5967,), (5967,)) . yrandom, ytrain . (array([-1.7 , 1.00830886, -4.45042772, ..., -2.56313048, -1.063 , -2.7813 ]), array([-5.05611784, -1.02322129, -8.7546501 , ..., -0.7956 , -1.11610783, -4.561 ])) . pipe.fit(xtrain,yrandom) preds_random = pipe.predict(xtest) . print(f&#39;RMSE (randomized target) test set = {mean_squared_error(preds_random, ytest, squared=False)}&#39;) . RMSE (randomized target) test set = 2.3878056084158237 . As you can see, the RMSE on the test set does increase. In a data set with dozens or even hundreds of features, which is relatively common in QSAR applications, we would expect much higher errors. That&#39;s why it&#39;s so important to analyze your features carefully in order to remove anything that could hamper performance, including missing or constant values and highly correlated features. . Feature selection . In our solubility dataset we have 17 features but we could have more. There are hundreds of molecular descriptors available in literature. So, how does one select the best set of features to include in a model? Let&#39;s investigate some feature selection methods! . 1) Filter methods . Filter methods are the easiest to implement because they are model-agnostic, which means we don&#39;t need any fancy learning algorithm. This class of methods consists of statistical approaches that use the distribution of the dataset to remove features that don&#39;t have much information or correlation with the target variable. Since we don&#39;t need a model, filter methods are very fast and can give an initial guess of what types of features to keep. . Despite being fast, filter methods also come with some dangerous drawbacks. Since most methods are univariate, important correlations between features may be missed; or worse redudant features could be selected. In addition, most metrics (e.g. Pearson&#39;s correlation coefficient) used to select the features are subjective and not always correlate with the metric that will be used to access performance. . While filter methods tend to be simple and fast, there is a subjective nature to the procedure. Most scoring methods have no obvious cut point to declare which predictors are important enough to go into the model. Even in the case of statistical hypothesis tests, the user must still select the confidence level to apply to the results. In practice, finding an appropriate value for the confidence value α may require several evaluations until acceptable performance is achieved . Applied Predictive Modeling, p.499. . 1.1) Basic approaches . Basic approaches are purely statistical and consits of removing constant or quasi-constant features from the dataset. . 1.1.1) Remove constant features using variance threshold . The first method we will use consists of removing constant or quasi-constant features. In this situation, all samples have the same value or almost the same. The intuition to remove this kind of feature is that it doesn&#39;t add any useful information to training because most values are not present or are repeated; so we can&#39;t differentiate between data points. . from sklearn.feature_selection import VarianceThreshold, mutual_info_regression . selector = VarianceThreshold() . scaler = RobustScaler() xnorm = scaler.fit(xtrain).transform(xtrain) . reducer = VarianceThreshold() reducer.fit(xnorm) . VarianceThreshold() . We can get the final number of features by using the get_support method from a fitted reducer. . sum(reducer.get_support()) . 17 . Well, it seems we don&#39;t have any feature with 0 variance. That&#39;s a good thing! But what about features with less than 1% variance? . reducer = VarianceThreshold(threshold=0.01) reducer.fit(xnorm) . VarianceThreshold(threshold=0.01) . sum(reducer.get_support()) . 17 . Again no feature was removed. That&#39;s a good start. You can play around with different threshold values depending on the dataset. . Let&#39;s try more robust methods! . 1.2) Univariate selection methods . Mutual information . Mutual information is a linear approach that quantifies the amount of information obtained about one random variable using another random variable. The mutual information is given by: . $$ begin{align} I(X; Y) = int_X int_Y p(x, y) log frac{p(x, y)}{p(x) p(y)} dx dy end{align} $$If the joint distribution $p(x,y)$ of variables $X$ and $Y$ equals the individual probabilities $p(x)$ and $p(y)$, the variables are considered independent and the integral is 0. Therefore, our goal is to find variables that are somehow correlated with the dependent variable logS that with maximum information content. . from sklearn.feature_selection import mutual_info_regression . importances = mutual_info_regression(xtrain, ytrain) . df_importances = pd.DataFrame(importances,index=descriptors) . df_importances.sort_values(0,ascending=False).head(5) . 0 . MolLogP 0.618195 | . MolWt 0.451355 | . MolMR 0.444390 | . LabuteASA 0.423057 | . NumValenceElectrons 0.366864 | . It seems that the top-5 most important features makes sense; as mentioned before logP and the molecular weight are important parameters to estimate water solubility. LabuteASA and NumValenceElectrons are also good estimates of the molecular shape and complexity. In fact, the top-5 features are what we expect based on the Pearson&#39;s R heatmap! That&#39;s very good, we can see the feature selection methods are showing some consistence and now we are bit more confident that we should include logP and some feature that encode structural information about the molecules. . For the bottom 5 features we can see that the are mostly related to atomic or fragment counts, which does tells us something about the structure but are not determinant for solubility per se. . df_importances.sort_values(0,ascending=False).tail(5) . 0 . NumRotatableBonds 0.080012 | . NumHeteroatoms 0.070472 | . NumHDonors 0.059317 | . NumAliphaticRings 0.024100 | . NumSaturatedRings 0.019100 | . We can also use the mutual information as metric in the sklearn SelectKBest class. . from sklearn.feature_selection import SelectKBest . kbest = SelectKBest(score_func=mutual_info_regression) xkbest = kbest.fit_transform(xtrain, ytrain) . The get_support method returns a bool array that we can use to index the list of descriptors. . np.array(descriptors)[kbest.get_support()] . array([&#39;MolWt&#39;, &#39;MolLogP&#39;, &#39;MolMR&#39;, &#39;HeavyAtomCount&#39;, &#39;NumValenceElectrons&#39;, &#39;NumAromaticRings&#39;, &#39;RingCount&#39;, &#39;LabuteASA&#39;, &#39;BalabanJ&#39;, &#39;BertzCT&#39;], dtype=&#39;&lt;U19&#39;) . In general, the most used approach is to use SelectKBest and select a scoring method, which will depend on the kind of variable we have. For continuous variables in regression problems, the f_regression and mutual_info_regression scoring functions are available on scikit-learn. For categorical variables in a classification problem, one can use Fischer chi-squared, mutual information for classification (mutual_info_classif) and the ANOVA F-value (f_classif) . 2) Wrapper methods . Wrapper methods consists of using models to decide if a feature should be added or removed. The main idea is to find a subset of features that maximize the performance. In this class of methods we have forward/backward selection and recursive feature elimination (RFE). Another way to understand wrapper methods is to think of them as search algorithms, with the features representing the search space and model performance as the target metric. . Wrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized. . Applied Predictive Modeling, p.499. . 2.1) Forward selection . In forward selection we start with 0 features and then add one at a time to evaluate the performance. If the performance on iteration $i+1$ is better than the previous $i^{th}$ iteration, we keep the new feature. The algorithm stops when the addition of new features does not lead to an increase in performance. . Let&#39;s implement a naive approach to forward selection. . from sklearn.linear_model import LinearRegression, Ridge from sklearn.svm import SVR from sklearn.neighbors import KNeighborsRegressor . pipe_ffs = Pipeline(steps=[(&#39;scaler&#39;,RobustScaler()), (&#39;estimator&#39;, LinearRegression())]) . ffs = ForwardSelection(pipe_ffs, xtrain, ytrain, scoring = partial(mean_squared_error, squared=False), test_size=0.25) . ffs.fit_predict() . Found import feature MolWt with new min RMSE of 1.756 Found import feature MolLogP with new min RMSE of 1.426 Found import feature MolMR with new min RMSE of 1.411 Found import feature HeavyAtomCount with new min RMSE of 1.396 Found import feature NumHeteroatoms with new min RMSE of 1.393 Found import feature NumRotatableBonds with new min RMSE of 1.340 Found import feature TPSA with new min RMSE of 1.339 Found import feature LabuteASA with new min RMSE of 1.338 Found import feature BalabanJ with new min RMSE of 1.338 . selected_features = np.array(descriptors)[ffs.get_support()] print(f&#39;Number of selected features = {len(selected_features)}&#39;) . Number of selected features = 9 . Our naive forward selector found 13 important features. If we go back to our heatmap, we can see that the selected features do have some correlation with solubility! Furthermore, it&#39;s basically the same features selected by the mutual information metric in the previous section. . Now let&#39;s try the faster and reliable scikit-learn implementation of forward selection, the SequentialFeatureSelector class. . We need to define a number of features to select so let&#39;s use 10 and compare the results with the approches so far. . from sklearn.feature_selection import SequentialFeatureSelector . ssf = SequentialFeatureSelector(estimator=pipe_ffs,n_features_to_select=12,cv=5, direction=&#39;forward&#39;,scoring=&#39;neg_mean_squared_error&#39;) . It seems SequentialFeatureSelector tries to maximize the scoring function. If we use the make_scorer(mean_squared_error) the selector will output features that increase the RMSE. Thus, we&#39;ll use the negative mean squared error, neg_mean_squared_error. . ssf.fit(xtrain, ytrain) . SequentialFeatureSelector(estimator=Pipeline(steps=[(&#39;scaler&#39;, RobustScaler()), (&#39;estimator&#39;, LinearRegression())]), n_features_to_select=12, scoring=&#39;neg_mean_squared_error&#39;) . selected_features_ssf = np.array(descriptors)[ssf.get_support()] . selected_features_ssf . array([&#39;MolLogP&#39;, &#39;MolMR&#39;, &#39;HeavyAtomCount&#39;, &#39;NumHAcceptors&#39;, &#39;NumHeteroatoms&#39;, &#39;NumRotatableBonds&#39;, &#39;NumValenceElectrons&#39;, &#39;NumSaturatedRings&#39;, &#39;RingCount&#39;, &#39;LabuteASA&#39;, &#39;BalabanJ&#39;, &#39;BertzCT&#39;], dtype=&#39;&lt;U19&#39;) . There we go! Our naive approache gives similar results to sklearn implementation. As always, MolLogP was selected as an informative feature. . 2.2) Backward selection . We can also do backward selection with SequentialFeatureSelector. In this method, we start with the whole set of features and remove at each step the ones that do not improve the model. . ssf_backward = SequentialFeatureSelector(estimator=pipe_ffs,n_features_to_select=0.5,cv=5, direction=&#39;backward&#39;,scoring=&#39;neg_mean_squared_error&#39;) . ssf_backward.fit(xtrain, ytrain) . SequentialFeatureSelector(direction=&#39;backward&#39;, estimator=Pipeline(steps=[(&#39;scaler&#39;, RobustScaler()), (&#39;estimator&#39;, LinearRegression())]), n_features_to_select=0.5, scoring=&#39;neg_mean_squared_error&#39;) . selected_features_ssf = np.array(descriptors)[ssf_backward.get_support()] print(selected_features_ssf) . [&#39;MolLogP&#39; &#39;HeavyAtomCount&#39; &#39;NumHeteroatoms&#39; &#39;NumRotatableBonds&#39; &#39;NumValenceElectrons&#39; &#39;LabuteASA&#39; &#39;BalabanJ&#39; &#39;BertzCT&#39;] . 2.3) Recursive feature elimination . Recursive feature elimination is another backward selection method but with the benefit of not fitting models using the same data multiple times. The first step consists of training the model on the full set of features. Then, the performance is calculated and the features are ranked in order of importance. The method iteratively removes features that are not important and rebuilds the model to estimate its performance and ranking of the remaining features. The process stops when a predefined number of features is reached. . In scikit-learn, we can use RFE as is or coupled with cross-validation (RFECV), in which case the mean cross-validated scored is returned. . from sklearn.feature_selection import RFE, RFECV . xnorm = scaler.fit_transform(xtrain) rfe = RFECV(estimator=LinearRegression(), cv=5, step=1) . rfe.fit(xnorm, ytrain) . RFECV(cv=5, estimator=LinearRegression()) . np.array(descriptors)[rfe.support_], rfe.ranking_ . (array([&#39;MolWt&#39;, &#39;MolLogP&#39;, &#39;MolMR&#39;, &#39;HeavyAtomCount&#39;, &#39;NumHDonors&#39;, &#39;NumHeteroatoms&#39;, &#39;NumRotatableBonds&#39;, &#39;NumValenceElectrons&#39;, &#39;RingCount&#39;, &#39;TPSA&#39;, &#39;LabuteASA&#39;, &#39;BalabanJ&#39;, &#39;BertzCT&#39;], dtype=&#39;&lt;U19&#39;), array([1, 1, 1, 1, 5, 1, 1, 1, 1, 2, 4, 3, 1, 1, 1, 1, 1])) . Again, consistent results with the previous selection methods. . 3) Intrinsic methods - selecting directly from a model . In scikit-learn, models that returns coef_ or feature_importances_ assigns importances to the features and we can use these models to select an optimal subset of features for training. . 3.1) Random Forest . Random forest is a learning algorithm that uses the average response of an ensemble of decision trees to make a prediction. Each tree considers a random subset of the features in order to make its predictions. At each node, the algorithm tries to minimize the impurity of a split or maximize its gain, which is similar to reducing the number of wrong predictions. We can use the average decrease in impurity of the trees at each split to rank the features the model considered most important to make its decision. In scikit-learn, the importances of fitted random forest can be accessed via the feature_importance_ attribute. . from sklearn.ensemble import RandomForestRegressor . rf = RandomForestRegressor(n_estimators=2000, n_jobs=-1) . rf.fit(xtrain, ytrain) . RandomForestRegressor(n_estimators=2000, n_jobs=-1) . importances = rf.feature_importances_.reshape(1,-1) . def rf_feat_importance(model, feats_names): return pd.DataFrame({&#39;cols&#39;:feats_names, &#39;imp&#39;:model.feature_importances_}).sort_values(&#39;imp&#39;,ascending=False) . fi = rf_feat_importance(rf, descriptors) . sns.barplot(y=&#39;cols&#39;, x=&#39;imp&#39;,data=fi) . &lt;AxesSubplot:xlabel=&#39;imp&#39;, ylabel=&#39;cols&#39;&gt; . . The relative importance is normalized to sum up to 1.0. Features with higher importances are assigned higher values. Thus, MolLogP was the most relevant feature for random forest, followed by BertzCT and BalabanJ (both are topological descriptors). Features based on counts of atoms or fragments were assiged very small importances, which means that we could probably remove than from the data and without impacting performance so much. . 3.2) LASSO . LASSO is a linear model trained with L1 regularization in order to reduce model complexity. By &quot;reduce complexity&quot; I mean make the model simpler to gain a boost in generalization. LASSO achieves regularization by using the L1 norm: . $$ min_{w} { frac{1}{2n_{ text{samples}}} ||X w - y||_2 ^ 2 + alpha ||w||_1}$$ . The L1 norm consists of adding the sum of the absolute values of the weights or coefficients to the cost function (e.g. sum of squared errors in a regression problem). In practice regularization forces the model to learn smaller weights, which is a way of making it simpler. Under L1 norm, most weights will be zero and as a result the weight vector will be sparse. Thus, the sparcitiy of the feature vector makes LASSO a great tool for feature selection since most unimportant features will be zeroed out. . Let&#39;s try scikit-learn implementation of LASSO. . from sklearn.linear_model import Lasso . The alpha hyperparameter is a penalization term. If alpha = 0, LASSO becomes ordinary least square (e.g. LinearRegression). The default value is 1.0 but we will reduce it in order to make the model more flexible and not zero out all features. . pipe_lasso = Pipeline(steps=[(&#39;scaler&#39;,RobustScaler()), (&#39;estimator&#39;, Lasso(alpha=0.1))]) . pipe_lasso.fit(xtrain, ytrain) . Pipeline(steps=[(&#39;scaler&#39;, RobustScaler()), (&#39;estimator&#39;, Lasso(alpha=0.1))]) . We can access the coefficients of the lasso estimator via the coef_ attribute of the pipeline. . feature_coef = pipe_lasso.named_steps.estimator.coef_ . feature_coef . array([-0.0024517 , -1.46516244, -0. , -0. , -0. , 0. , -0.05151828, 0.09117961, -0. , -0. , -0. , -0. , -0. , -0. , -0. , 0. , -0.56361338]) . That&#39;s how the magic happens! The LASSO esimator zeroed out most features. It seems only 5 features survived the ordeal. . np.array(descriptors)[np.where(feature_coef!=0.)[0]] . array([&#39;MolWt&#39;, &#39;MolLogP&#39;, &#39;NumHeteroatoms&#39;, &#39;NumRotatableBonds&#39;, &#39;BertzCT&#39;], dtype=&#39;&lt;U19&#39;) . I&#39;m not surprised at all! We can see features that were also selected by some of the previous methods. . Final selection of features . Let&#39;s review which features were selected by the different approaches. Overall, the feature selection methods seems to agree that MolLogP and MolWt are important for predicting solubility; so let&#39;s keep them. The same applies for the number of rotatable bounds; in general flexibility reduce solubility because it makes more difficult for molecules to pack together and establish intermolecular interactions. . LabuteASA might also be useful because it gives information about the shape and size of the molecule. However, it&#39;s highly correlated with MolWt and MolMR, besides giving similar information about molecular size to these features. Furthermore, if we consider the importances returned by random forest and LASSO, we can see that LabuteASA have a very low importance. . At least one topological descriptor, BalabanJ and BertzCT, was selected in most methods. The information they encode have a relative overlap, so it could be worth investigating which is more informative for us. We could also keep both features since they don&#39;t have a high collinearity. . Only RFE and random forest recognized the importance of TPSA to predict solubility. In fact, TPSA encodes the H-bonding potential of a molecule because it includes all nitrogen, oxygen and sulphur atoms, making count-based descriptors not so important (e.g. NumHBondDonors and NumHBondAcceptors). Thus, we could consider including TPSA since it can give us implicit information about intermolecular interactions and charge distribution. . Let&#39;s rebuild the model using the reduced set of features. . descriptors_reduced = [&#39;MolWt&#39;,&#39;MolLogP&#39;,&#39;TPSA&#39;,&#39;BalabanJ&#39;,&#39;BertzCT&#39;,&#39;MolMR&#39;,&#39;NumRotatableBonds&#39;] xtrain_reduced = trainset[descriptors_reduced].values xtest_reduced = testset[descriptors_reduced].values . rf.fit(xtrain_reduced, ytrain) . RandomForestRegressor(n_estimators=2000, n_jobs=-1) . preds_reduced = rf.predict(xtest_reduced) print(f&#39;RMSE (reduced set) test set = {mean_squared_error(preds_reduced, ytest, squared=False)}&#39;) . RMSE (reduced set) test set = 0.979472446585308 . As we can see, the performance is almost the same as the baseline trained with all features. . What method should I use? . In this post we used different feature selection methods and learned about their advantages and drawbacks. So, which method is the best? The answer to this is a good o&#39; &quot;it depends&quot;. The best method depends on the type of feature and problem that you have. The image below will help you select a method and the reader is referred to the feature selection post at machine learning mastery. . You don&#39;t need to use all feature selection methods on your dataset! . . Figure 3. Feature selection strategies . Conclusion . We learned how to inspect the correlation between features using heatmap and select the most informative subset of features using selection methods. The most important takeaway is that irrelevant features must be removed before training a model. If a feature do not contain relevant information or is highly correlated with other features, it will only make the model more complex, less intepretable and you risk reducing the generalization potential of the model. . In the next post we will train our solubility prediction model using the selected features. Can we achieve state-of-the art performance? Stay tuned for the next posts! . References . Molecular descriptors . https://northstar-www.dartmouth.edu/doc/MOE/Documentation/quasar/descr.htm#KH . http://www.codessa-pro.com/descriptors/index.htm . Feature selection methods . https://www.kaggle.com/prashant111/comprehensive-guide-on-feature-selection#5.-How-to-choose-the-right-feature-selection-method- . https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/ . Kuhn, Max., and Kjell Johnson. Applied Predictive Modeling. New York: Springer, 2013. . https://towardsdatascience.com/mistakes-in-applying-univariate-feature-selection-methods-34c43ce8b93d . https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection . https://scikit-learn.org/stable/modules/feature_selection.html . Fin .",
            "url": "https://marcossantanaioc.github.io/fiocruzcheminformatics/fastpages/jupyter/2021/06/06/feature_selection.html",
            "relUrl": "/fastpages/jupyter/2021/06/06/feature_selection.html",
            "date": " • Jun 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Predicting water solubility - Part I?",
            "content": "Water solubility is one onf the main players in lead optimization. If a molecule is not soluble, we might have problems in biological assays and also to make it reach the desired target in vivo. . This post is the first of a series that will end with a prediction model to estimate water solubility. In this first notebook, we will see some exploratory data analysis and how to prepare a dataset for modeling. . Requirements . rdkit &gt;= 2020.09.1 | pandas &gt;= 1.1.3 | seaborn | matplotlib | fastcore (!conda install fastcore) | . In this tutorial we&#39;ll use a dataset compiled by Sorkun et al (2019) from multiple projects to predict water solubility. You can download the original dataset from here. . At the end of the notebook I added a class to process the original dataset in order to remove salts, mixtures, neutralize charges and generate canonical SMILES. I highly recommend checking each structure before modeling. . Import modules . %reload_ext autoreload %autoreload 2 %matplotlib inline from IPython.display import Image import matplotlib.pyplot as plt from matplotlib.ticker import FormatStrFormatter import seaborn as sns import pandas as pd import numpy as np from sklearn.preprocessing import Normalizer, normalize, RobustScaler from functools import partial from pathlib import Path from joblib import load, dump from scipy import stats from scipy.stats import norm from statsmodels.graphics.gofplots import qqplot from rdkit.Chem import Draw from rdkit.Chem import MolFromSmiles, MolToSmiles . . np.random.seed(5) . sns.set(rc={&#39;figure.figsize&#39;: (16, 16)}) sns.set_style(&#39;whitegrid&#39;) sns.set_context(&#39;paper&#39;,font_scale=1.5) . Load Data . data = pd.read_csv(&#39;../_data/curated-solubility-dataset_processed.csv&#39;) . data.head() . ID Name InChI InChIKey SMILES Solubility SD Ocurrences Group MolWt ... NumValenceElectrons NumAromaticRings NumSaturatedRings NumAliphaticRings RingCount TPSA LabuteASA BalabanJ BertzCT processed_smiles . 0 A-3 | N,N,N-trimethyloctadecan-1-aminium bromide | InChI=1S/C21H46N.BrH/c1-5-6-7-8-9-10-11-12-13-... | SZEMGTQCPRNXEG-UHFFFAOYSA-M | [Br-].CCCCCCCCCCCCCCCCCC[N+](C)(C)C | -3.616127 | 0.0 | 1 | G1 | 392.510 | ... | 142.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.00 | 158.520601 | 0.000000 | 210.377334 | CCCCCCCCCCCCCCCCCC[N+](C)(C)C | . 1 A-4 | Benzo[cd]indol-2(1H)-one | InChI=1S/C11H7NO/c13-11-8-5-1-3-7-4-2-6-9(12-1... | GPYLCFQEKPUWLD-UHFFFAOYSA-N | O=C1Nc2cccc3cccc1c23 | -3.254767 | 0.0 | 1 | G1 | 169.183 | ... | 62.0 | 2.0 | 0.0 | 1.0 | 3.0 | 29.10 | 75.183563 | 2.582996 | 511.229248 | O=C1Nc2cccc3cccc1c23 | . 2 A-5 | 4-chlorobenzaldehyde | InChI=1S/C7H5ClO/c8-7-3-1-6(5-9)2-4-7/h1-5H | AVPYQKSLYISFPO-UHFFFAOYSA-N | Clc1ccc(C=O)cc1 | -2.177078 | 0.0 | 1 | G1 | 140.569 | ... | 46.0 | 1.0 | 0.0 | 0.0 | 1.0 | 17.07 | 58.261134 | 3.009782 | 202.661065 | O=Cc1ccc(Cl)cc1 | . 3 A-9 | 4-({4-[bis(oxiran-2-ylmethyl)amino]phenyl}meth... | InChI=1S/C25H30N2O4/c1-5-20(26(10-22-14-28-22)... | FAUAZXVRLVIARB-UHFFFAOYSA-N | C1OC1CN(CC2CO2)c3ccc(Cc4ccc(cc4)N(CC5CO5)CC6CO... | -4.662065 | 0.0 | 1 | G1 | 422.525 | ... | 164.0 | 2.0 | 4.0 | 4.0 | 6.0 | 56.60 | 183.183268 | 1.084427 | 769.899934 | c1cc(N(CC2CO2)CC2CO2)ccc1Cc1ccc(N(CC2CO2)CC2CO... | . 4 A-10 | vinyltoluene | InChI=1S/C9H10/c1-3-9-6-4-5-8(2)7-9/h3-7H,1H2,2H3 | JZHGRUMIRATHIU-UHFFFAOYSA-N | Cc1cccc(C=C)c1 | -3.123150 | 0.0 | 1 | G1 | 118.179 | ... | 46.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.00 | 55.836626 | 3.070761 | 211.033225 | C=Cc1cccc(C)c1 | . 5 rows × 27 columns . . Exploratory data analysis . In this notebook we&#39;ll cover three main topics: . What&#39;s the relationship between our features and solubility? | What&#39;s the distribution of our target variable? | Are there outliers and how can we detect them? | . Preliminary analysis . Let&#39;s start by counting how many features we have. In total we have 17 descriptors already calculated for our data set. . descriptors = [&#39;MolWt&#39;, &#39;MolLogP&#39;, &#39;MolMR&#39;, &#39;HeavyAtomCount&#39;, &#39;NumHAcceptors&#39;, &#39;NumHDonors&#39;, &#39;NumHeteroatoms&#39;, &#39;NumRotatableBonds&#39;, &#39;NumValenceElectrons&#39;, &#39;NumAromaticRings&#39;, &#39;NumSaturatedRings&#39;, &#39;NumAliphaticRings&#39;, &#39;RingCount&#39;, &#39;TPSA&#39;, &#39;LabuteASA&#39;, &#39;BalabanJ&#39;, &#39;BertzCT&#39;] . print(len(descriptors)) . 17 . We can check the correlation between each feature and solubility using seaborn&#39;s regplot: . nr_rows = 6 nr_cols = 3 target = &#39;Solubility&#39; . fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3)) for r in range(0,nr_rows): for c in range(0,nr_cols): i = r*nr_cols+c if i &lt; len(descriptors): sns.regplot(x=data[descriptors[i]], y=data[target], ax = axs[r][c]) stp = stats.pearsonr(data[descriptors[i]], data[target]) str_title = &quot;r = &quot; + &quot;{0:.2f}&quot;.format(stp[0]) + &quot; &quot; &quot;p = &quot; + &quot;{0:.2f}&quot;.format(stp[1]) axs[r][c].set_title(str_title,fontsize=11) plt.tight_layout() plt.show() . Most have a negative correlation with solubility. In fact, the available features reflect more or less the size/complexity of the molecule (e.g. MolWt, RingCount, MolMR) and it&#39;s capacity to make inter and intramolecular interactions (e.g., NumValenceElectrons, NumHAcceptors, NumHDonors, MolLogP), which are known to influence water solubility. For instance, bigger molecules tend to be less soluble in water; the same applies for very liphophilic (aka aversion to water) molecules. On the other hand, more polar molecules, capable of making hydrogen bonds with water molecules, are usually more soluble. . Based on this preliminary analysis we can start thinking which features to include when training a regression model. . Reliability of data . Sorkun et al. described in their paper that the molecules in the dataset were classified according to the number of occurence and the standard deviation of logS measurements. Based on this, the authors considered 5 groups: G1-5, with G5 showing the highest reliability (+3 occurences and SD &lt; 0.5), followed by G4 (+3 occurences and SD &gt; 0.5), G3 (occurences = 2 and SD &lt; 0.5) and G2 (occurences = 2 and SD &gt; 0.5), and finally G1 (just one occurance in all datasets). . Compounds can be classified according to solubility values (LogS); Compounds with 0 and higher solubility value are highly soluble, those in the range of 0 to −2 are soluble, those in the range of −2 to −4 are slightly soluble and insoluble if less than −4. . data[&#39;class&#39;] = None data.loc[data[&#39;Solubility&#39;]&gt;0.0, &#39;class&#39;] = &#39;highly soluble&#39; data.loc[(data[&#39;Solubility&#39;]&gt;-2) &amp; (data[&#39;Solubility&#39;]&lt;=0), &#39;class&#39;] = &#39;soluble&#39; data.loc[(data[&#39;Solubility&#39;]&gt;-4.0) &amp; (data[&#39;Solubility&#39;]&lt;=-2.0), &#39;class&#39;] = &#39;slightly soluble&#39; data.loc[data[&#39;Solubility&#39;]&lt;=-4.0, &#39;class&#39;] = &#39;insoluble&#39; . Let&#39;s count the molecules in each reliability group. . counts = data.groupby([&#39;Group&#39;,&#39;class&#39;])[&#39;ID&#39;].count().reset_index() counts . Group class ID . 0 G1 | highly soluble | 624 | . 1 G1 | insoluble | 1976 | . 2 G1 | slightly soluble | 2364 | . 3 G1 | soluble | 1929 | . 4 G2 | highly soluble | 11 | . 5 G2 | insoluble | 114 | . 6 G2 | slightly soluble | 51 | . 7 G2 | soluble | 29 | . 8 G3 | highly soluble | 90 | . 9 G3 | insoluble | 305 | . 10 G3 | slightly soluble | 353 | . 11 G3 | soluble | 324 | . 12 G4 | highly soluble | 10 | . 13 G4 | insoluble | 44 | . 14 G4 | slightly soluble | 46 | . 15 G4 | soluble | 21 | . 16 G5 | highly soluble | 40 | . 17 G5 | insoluble | 140 | . 18 G5 | slightly soluble | 194 | . 19 G5 | soluble | 177 | . sns.barplot(x=&#39;Group&#39;,y=&#39;ID&#39;,data=counts,hue=&#39;class&#39;,ci=95) . &lt;AxesSubplot:xlabel=&#39;Group&#39;, ylabel=&#39;ID&#39;&gt; . The G1 group dominates, with the majority of molecules, followed by G3 and G5. G4 and G2 are the smallest groups, and contains mostly soluble or slightly soluble molecules. The number of insoluble molecules in all groups is high and comparable to the number of soluble and slighly soluble. In addition, for all groups the number of highly soluble molecules is small; in G2 these molecules represent only 5%. . This initial analysis already shows us that our distribution probably is skewed towards less soluble molecules. This might be a problem when training our model because it might make more errors in the chemical space of highly soluble molecules, since they are not well represented in the dataset. . Summary statistics . data.describe() . Solubility SD Ocurrences MolWt MolLogP MolMR HeavyAtomCount NumHAcceptors NumHDonors NumHeteroatoms NumRotatableBonds NumValenceElectrons NumAromaticRings NumSaturatedRings NumAliphaticRings RingCount TPSA LabuteASA BalabanJ BertzCT . count 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | 8842.000000 | . mean -2.976133 | 0.059580 | 1.345397 | 253.005300 | 2.445421 | 66.159777 | 16.904207 | 3.174169 | 1.097263 | 4.667496 | 3.927279 | 92.041280 | 1.064917 | 0.318706 | 0.467541 | 1.532459 | 55.691938 | 103.250839 | 2.618086 | 443.741261 | . std 2.315261 | 0.205385 | 0.809062 | 150.964318 | 2.811272 | 41.035370 | 10.507310 | 2.693266 | 1.433689 | 3.480039 | 5.265856 | 56.843391 | 1.194696 | 0.910786 | 1.080055 | 1.528568 | 46.990709 | 62.039117 | 0.819560 | 436.470929 | . min -13.171900 | 0.000000 | 1.000000 | 26.038000 | -17.406400 | 3.450000 | 2.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 10.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 11.307062 | -0.000004 | 0.000000 | . 25% -4.346725 | 0.000000 | 1.000000 | 163.176000 | 0.957900 | 42.360000 | 11.000000 | 2.000000 | 0.000000 | 2.000000 | 1.000000 | 60.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 26.020000 | 67.199259 | 2.169297 | 183.391645 | . 50% -2.689542 | 0.000000 | 1.000000 | 226.446000 | 2.141950 | 59.451150 | 15.000000 | 3.000000 | 1.000000 | 4.000000 | 3.000000 | 82.000000 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | 48.560000 | 92.834705 | 2.622906 | 368.308634 | . 75% -1.366225 | 0.000000 | 1.000000 | 310.365500 | 3.555000 | 81.085350 | 20.000000 | 4.000000 | 2.000000 | 6.000000 | 5.000000 | 110.000000 | 2.000000 | 0.000000 | 1.000000 | 2.000000 | 75.270000 | 126.296571 | 3.069899 | 606.196178 | . max 2.137682 | 3.270214 | 12.000000 | 5299.456000 | 68.541140 | 1419.351700 | 388.000000 | 86.000000 | 26.000000 | 89.000000 | 141.000000 | 2012.000000 | 35.000000 | 30.000000 | 30.000000 | 36.000000 | 1214.340000 | 2230.685124 | 7.517310 | 20720.267708 | . Let&#39;s take a look at the pairplot to see how the features interact with each other and with our target. . sns.pairplot(data) . &lt;seaborn.axisgrid.PairGrid at 0x7f0dd6fafcd0&gt; . . We can see some interesting information about this dataset. Our target have a normal-like distribution (first plot on the top left). This is very good because some algorithms assume that the data follows a normal distribution. If our data is normal it should be easier to train the model. . Normality check . Here&#39;s another view of the target solubility variable using violin and kernel density plots. The plots shows a negative skew (the distribution shifted to the right), indicating that most molecules are not highly soluble. . fig, (ax1, ax2) = plt.subplots(2,figsize=(12,8)) sns.set_context(font_scale=2) sns.histplot(x=&#39;Solubility&#39;,data=data,ax=ax1,hue=&#39;class&#39;) ax1.axvline(data[&#39;Solubility&#39;].mean(), color=&#39;k&#39;, linestyle=&#39;dashed&#39;, linewidth=2.5) sns.violinplot(data[&#39;Solubility&#39;].values,ax=ax2) # set the spacing between subplots plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4) ax1.title.set_text(&#39;Histogram&#39;) ax2.title.set_text(&#39;Violin plot&#39;) plt.ylabel(&#39;Count&#39;,fontsize=18) plt.xlabel(&#39;Solubility&#39;,fontsize=18) . /home/marcossantana/anaconda3/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . Text(0.5, 0, &#39;Solubility&#39;) . In order to gain a better insight into our distribution, let&#39;s calculate how big is that negative skew and how much the distribution deviates from a Gaussian (normal) distribution. We&#39;ll use two values: skewness and kurtosis . Skewness and kurtosis can be defined as follows: . Skew is how much a distribution is pushed left or right, a measure of asymmetry in the distribution. We can think about skew as if someone was pushing the tail of a normal distribution. | . Kurtosis quantifies how much of the distribution is in the tail. It is a simple and commonly used statistical test for normality. A good analogy is to think about punching the peak or top of the distribution and making it spread to the tails. | . We can calculate skewness and kurtosis using scipy skew and kurtosis functions, respectively. . . Figure 1. Skewness . . Figure 2. Kurtosis . from scipy.stats import skew, kurtosis, kurtosistest, skewtest skew_val = skew(data[&#39;Solubility&#39;]) kurtosis_val = kurtosis(data[&#39;Solubility&#39;]) print(f&#39;Skewness = {skew_val}&#39;) print(f&#39;Kurtosis = {kurtosis_val}&#39;) . Skewness = -0.5593279897021967 Kurtosis = 0.04726151580435545 . Now let&#39;s confirm the hypothesis that our distribution is not normal. by running different normality tests, such as D&#39;Agostino-Pearson , Shapiro-Wilk and Anderson tests. . from scipy.stats import normaltest, shapiro, anderson . The D&#39;Agostino-Pearson normality test measures how far a distribution is from normality by computing skew and kurtosis. In this test, the null hypothesis (H0) is that our distribution is normal. We can use the k2 statistic and the p-value to reject or not H0. . print(&quot;D&#39;Agostino K2&quot;) k2, p = normaltest(data[&#39;Solubility&#39;]) print(f&#39;Statistics={k2}, p={p}&#39;) # interpret alpha = 0.05 if p &gt; alpha: print(&#39;Sample looks Gaussian (fail to reject H0)&#39;) else: print(&#39;Sample does not look Gaussian (reject H0)&#39;) . D&#39;Agostino K2 Statistics=404.5266797956529, p=1.4392876240350922e-88 Sample does not look Gaussian (reject H0) . The Shapiro-Wilk normality test calculates a W statistics to check if a sample comes from a normal distribution. In this test, the null hypothesis (H0) is that our distribution is normal. We can use the W statistic and the p-value to reject or not H0. . print(&quot;Shapiro-Wilk&quot;) w_stat, p = shapiro(data[&#39;Solubility&#39;]) print(f&#39;Statistics={w_stat}, p={p}&#39;) # interpret alpha = 0.05 if p &gt; alpha: print(&#39;Sample looks Gaussian (fail to reject H0)&#39;) else: print(&#39;Sample does not look Gaussian (reject H0)&#39;) . Shapiro-Wilk Statistics=0.9754287600517273, p=2.2349054059092123e-36 Sample does not look Gaussian (reject H0) . /home/marcossantana/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:1681: UserWarning: p-value may not be accurate for N &gt; 5000. warnings.warn(&#34;p-value may not be accurate for N &gt; 5000.&#34;) . The Anderson-Darling normality test measures if a sample comes from a specific distribution (e.g. normal). It measures the distance between the cumulative probability distribution between the sample and a specific distribution. The test is a modified version of the nonparametric goodness-of-fit Kolmogorov-Smirnov test. In this test, the null hypothesis (H0) is that our distribution is normal. We can use the A statistic and the p-value to reject or not H0. In reality, the scipy function returns a range of critical values that allows us to interpret if H0 should be rejected. . print(&quot;Anderson-Darling&quot;) result = anderson(data[&#39;Solubility&#39;]) print(f&#39;A statistic = {result.statistic}&#39;) p = 0 for i in range(len(result.critical_values)): sl, cv = result.significance_level[i], result.critical_values[i] if result.statistic &lt; result.critical_values[i]: print(f&#39;{sl:.3f}: {cv:.3f}, data looks normal (fail to reject H0)&#39;) else: print(f&#39;{sl:.3f}: {cv:.3f}, data does not look normal (reject H0)&#39;) . Anderson-Darling A statistic = 54.56324161518205 15.000: 0.576, data does not look normal (reject H0) 10.000: 0.656, data does not look normal (reject H0) 5.000: 0.787, data does not look normal (reject H0) 2.500: 0.918, data does not look normal (reject H0) 1.000: 1.092, data does not look normal (reject H0) . We can also plot some features to investigate their distribution. . fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(12,12)) sns.histplot(data[&#39;MolWt&#39;],ax=ax1) sns.histplot(data[&#39;MolLogP&#39;],ax=ax2) sns.histplot(data[&#39;TPSA&#39;],ax=ax3) sns.histplot(data[&#39;BertzCT&#39;],ax=ax4) # set the spacing between subplots plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4) ax1.set_title(&#39;Molecular weight (Da)&#39;) ax2.set_title(&#39;Lipophilicity (logP)&#39;) ax3.set_title(&#39;Total polar surface area (TPSA)&#39;) ax4.set_title(&#39;BertzCT descriptor&#39;) . Text(0.5, 1.0, &#39;BertzCT descriptor&#39;) . Molecular weight (MolWt), Lipophilicity (MolLogP), total polar surface area (TPSA) and the BertzCT topological descriptor show a clear positive skew. These distributions actually make sense based on our observations about solubility. Most of the molecules in the dataset are in the slightly soluble - soluble range (Solubility between -4 to 0), and as we can see from the above plots, there is a positive skew on molecular weight and lipophilicity. . Detecting outliers . We can use the information we have to identify data points that are very different from the general distribution and that could hinder the performance of a predictive model. These data points are called outliers and their presence is very bad for many type of algorithms, leading to longer training time, more complex models and maybe degraded performance. . fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3)) for r in range(0,nr_rows): for c in range(0,nr_cols): i = r*nr_cols+c if i &lt; len(descriptors): sns.scatterplot(x=data[descriptors[i]], y=data[target], ax = axs[r][c]) axs[r][c].set_title(descriptors[i],fontsize=11) plt.tight_layout() plt.show() . The scatter plots shows us that some points are distant from the others. An extreme case is a data point on the top right of the plots, showing high molecular weight, TPSA and logP! It does seem we have some outliers in our data. But how can we detect which molecules are outliers? Can we simply remove them based on some kind of rule of thumb? Let&#39;s investigate this! . Using mean and std to find outliers . The first outlier detection method we&#39;ll use is based on the mean and standard deviation of our data. In this method, a point is considered outlier if it&#39;s 2 std below or above the mean of our sample. . c = 2 def get_outliers(data, feature_name, c=2): mean, std = data[feature_name].mean(),data[feature_name].std() bounds = mean - (c*std), mean + (c*std) outliers = data[~data[feature_name].between(*bounds)] non_outliers = data.iloc[data.index.difference(outliers.index)] return bounds, outliers, non_outliers . outliers = [] for i in descriptors: outliers.append(get_outliers(data, feature_name= i, c=2)[1]) . fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3)) for r in range(0,nr_rows): for c in range(0,nr_cols): i = r*nr_cols+c if i &lt; len(descriptors): sns.scatterplot(x=data[descriptors[i]], y=data[target], ax = axs[r][c]) sns.scatterplot(x=outliers[i][descriptors[i]], y=outliers[i][target], ax = axs[r][c]) # print(f&#39;Possible number of outliers = {}&#39;) axs[r][c].set_title(f&#39;{descriptors[i]} nNumber of outliers = {len(outliers[i])}&#39;,fontsize=11) # set the spacing between subplots plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4) plt.tight_layout() plt.show() . The scatterplots shows some potential outliers for each feature. . Let&#39;s take a closer look at solubility and one of the descriptor to get some insight of physicochemical properties of potential outliers. . Water solubility has a strong negative correlation with lipophilicity (logP). Ideally, we should see a solubility decrease for more lipophilic molecules. Let&#39;s try using logP to identify potential outliers. . logp_bonds,logp_outliers,logp_non_outliers = get_outliers(data, feature_name= &#39;MolLogP&#39;, c=2) . print(logp_bonds) . (-3.1771227601854855, 8.067965494860898) . print(f&#39;Possible number of outliers = {len(logp_outliers)}&#39;) . Possible number of outliers = 300 . We found 300 potential outliers that differ from the mean MolLogP by 2 std. We can clearly see the extreme point we identified earlier and some other more distant fromt he distribution. . Here&#39;s MolLogP before and after outliers . fig, ax1 = plt.subplots(1,figsize=(20,16), sharey=True) s=90 sns.scatterplot(x=&#39;MolLogP&#39;, y=&#39;Solubility&#39;, data =logp_non_outliers,ax=ax1,s=s,color=&#39;gray&#39;) sns.scatterplot(x=&#39;MolLogP&#39;, y=&#39;Solubility&#39;, data =logp_outliers,ax=ax1,color=&#39;goldenrod&#39;,s=s) ax1.set_title(&#39;MolLogP outliers&#39;,fontsize=18) ax1.set_ylabel(&#39;Solubility&#39;,fontsize=18) ax1.set_xlabel(&#39;MolLogP&#39;,fontsize=18) #sns.scattrplot(x=&#39;MolLogP&#39;, y=&#39;Solubility&#39;, data = logp_non_outliers,ax=ax3,color=&#39;gray&#39;) #ax3.set_title(&#39;Removed outliers&#39;) . Text(0.5, 0, &#39;MolLogP&#39;) . . If we remove the putative outliers we can get a better view of the relationship between logP and solubility. . fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(36,12), sharey=True) s=90 sns.scatterplot(x=&#39;MolLogP&#39;, y=&#39;Solubility&#39;, data =data,ax=ax1,s=s,color=&#39;gray&#39;) ax1.set_title(&#39;Original data&#39;) sns.scatterplot(x=&#39;MolLogP&#39;, y=&#39;Solubility&#39;, data =logp_non_outliers,ax=ax2,s=s,color=&#39;gray&#39;) sns.scatterplot(x=&#39;MolLogP&#39;, y=&#39;Solubility&#39;, data =logp_outliers,ax=ax2,color=&#39;goldenrod&#39;,s=s) ax2.set_title(&#39;Outliers&#39;) sns.scatterplot(x=&#39;MolLogP&#39;, y=&#39;Solubility&#39;, data = logp_non_outliers,ax=ax3,color=&#39;gray&#39;) ax3.set_title(&#39;Removed outliers&#39;) . Text(0.5, 1.0, &#39;Removed outliers&#39;) . The scatterplot on the right shows that as logP increases, the solubility tends to decrease, which is what we expected. In general, very big compounds are less soluble and have a hard time being optimized into lead compounds or drugs. But what kind of molecules did we remove? . from rdkit.Chem import Draw . Draw.MolsToGridImage(list(map(MolFromSmiles, logp_outliers.processed_smiles)),subImgSize=(900,900)) . /home/marcossantana/anaconda3/lib/python3.7/site-packages/rdkit/Chem/Draw/IPythonConsole.py:192: UserWarning: Truncating the list of molecules to be displayed to 50. Change the maxMols value to display more. % (maxMols)) . . Some potential outliers doesn&#39;t look very friendly or something that would be in drug discovery libraries! Take a look at this guy: . smi = logp_outliers.processed_smiles.tolist()[2] mol = MolFromSmiles(smi) print(f&#39;Outlier SMILES - {smi}&#39;) mol . Outlier SMILES - CCCCCCCCCCCCCCCCCC(=O)OCC(COC(=O)CCCCCCCCCCCCCCCCC)OC(=O)CCCCCCCCCCCCCCCCC . . We might be better off without this kind of &quot;ugly chemistry&quot;. But let&#39;s keep these monsters for now and explore other methods to outlier detection. . Interquartile range . We can also use the interquartile range (IQR) to detect outliers. Simply put, we&#39;ll use a boxplot to detect molecules with values higher than the 75th quartile and lower than the 25th quartile. . def iqr_detection(data, feature_name, std_cut=1.5): df = data.copy() df.reset_index(drop=True,inplace=True) q3 = np.quantile(df[feature_name],.75) q1 = np.quantile(df[feature_name],.25) iqr = q3 - q1 cutoff = std_cut * iqr lower, upper = q1 - cutoff, q3 + cutoff outliers = df[(df[feature_name] &lt; lower) | (df[feature_name] &gt; upper)] ok_points = df.iloc[df.index.difference(outliers.index)] return (lower, upper), outliers, ok_points . We will consider a data point as outlier if it lies at above or below the cutoff value of 1.5 * IQR . iqr_bounds, iqr_outliers, iqr_okpoints = iqr_detection(data, &#39;Solubility&#39;) . print(iqr_bounds) . (-8.817475000000002, 3.1045250000000006) . Automatic detection . We can also automate the detection of outliers using the local outlier factor algorithm . The local outlier factor (LOF) adopts a notion of oulier that depends on the local neighborhood of each data point. This allows the algorithm to find data points that are different from the global distribution and also different from their neighbors. LOF does this by assigning an outlier factor to each data point, which is the degree each point deviates from its neighbors. An outlier is expected to have a local densitiy that is smaller (its far away) from it&#39;s k-neighbors. . Normalize data . Let&#39;s try to find some outliers with the available descriptors. But first, let&#39;s normalize them! . X = data[descriptors].values Y = data[&#39;Solubility&#39;].values . Y.shape . (8842,) . scaler = RobustScaler() Xnorm = scaler.fit(X).transform(X) . Xnorm.mean(),Xnorm.std() . (0.176258938735513, 1.0250171066612603) . LocalOutlierFactor . from sklearn.neighbors import LocalOutlierFactor . outlier_scores = LocalOutlierFactor(n_neighbors=20,novelty=False).fit_predict(Xnorm) . lof_no_outliers = data.iloc[outlier_scores == 1] lof_outliers = data.iloc[outlier_scores == -1] lof_no_outliers.shape,lof_outliers.shape . ((8525, 28), (317, 28)) . Draw.MolsToGridImage(list(map(MolFromSmiles, lof_outliers.processed_smiles.tolist())),subImgSize=(600,600)) . /home/marcossantana/anaconda3/lib/python3.7/site-packages/rdkit/Chem/Draw/IPythonConsole.py:192: UserWarning: Truncating the list of molecules to be displayed to 50. Change the maxMols value to display more. % (maxMols)) . . It seems LOF removed the most extreme points. After removing outliers, we can see that the correlation between each feature and solubility increased. The only exception was TPSA, which showed a small decrease in the Pearson&#39;s correlation. . In the next posts, we&#39;ll explore more of this data. The main question we&#39;ll try to answer is: can we train a model with good performance to predict water solubility? . References . Detecting outliers . https://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html . https://www.dummies.com/programming/big-data/data-science/graphical-tests-of-data-outliers/ . https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/ . https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba . https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf . https://towardsdatascience.com/local-outlier-factor-for-anomaly-detection-cc0c770d2ebe . Normality tests . Machine learning mastery post: https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/ . Shapiro-Wilk test: https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm . Anderson-Darling test : https://www.itl.nist.gov/div898/handbook/prc/section2/prc21.htm . GraphPad entry: https://www.graphpad.com/guides/prism/latest/statistics/stat_choosing_a_normality_test.htm . Fin . Sample class to sanitize the dataset . from unittest.mock import * from rdkit import Chem from tqdm.notebook import tqdm from rdkit.Chem.Draw import IPythonConsole, MolsToGridImage from rdkit.Chem.SaltRemover import SaltRemover as saltremover from rdkit.Chem.MolStandardize.rdMolStandardize import LargestFragmentChooser from rdkit.Chem import AllChem, SanitizeMol from rdkit import rdBase from rdkit.Chem import rdmolops from fastcore.all import * rdBase.DisableLog(&#39;rdApp.error&#39;) rdBase.DisableLog(&#39;rdApp.info&#39;) . . _saltremover = saltremover() _unwanted = Chem.MolFromSmarts(&#39;[!#1!#6!#7!#8!#9!#15!#16!#17!#35!#53]&#39;) def _initialiseNeutralisationReactions(): patts = ( # Imidazoles (&#39;[n+;H]&#39;, &#39;n&#39;), # Amines (&#39;[N+;!H0]&#39;, &#39;N&#39;), # Carboxylic acids and alcohols (&#39;[$([O-]);!$([O-][#7])]&#39;, &#39;O&#39;), # Thiols (&#39;[S-;X1]&#39;, &#39;S&#39;), #Amidines (&#39;[C+](N)&#39;, &#39;C(=N)&#39;), # Sulfonamides (&#39;[$([N-;X2]S(=O)=O)]&#39;, &#39;N&#39;), # Enamines (&#39;[$([N-;X2][C,N]=C)]&#39;, &#39;N&#39;), # Tetrazoles (&#39;[n-]&#39;, &#39;[nH]&#39;), # Sulfoxides (&#39;[$([S-]=O)]&#39;, &#39;S&#39;), # Amides (&#39;[$([N-]C=O)]&#39;, &#39;N&#39;), ) return [(Chem.MolFromSmarts(x),Chem.MolFromSmiles(y,False)) for x,y in patts] def neutralise(mol,reactions=None): reactions = _initialiseNeutralisationReactions() replaced = False for i,(reactant, product) in enumerate(reactions): while mol.HasSubstructMatch(reactant): replaced = True rms = AllChem.ReplaceSubstructs(mol, reactant, product) mol = rms[0] if replaced: rdmolops.Cleanup(mol) rdmolops.SanitizeMol(mol) mol = rdmolops.RemoveHs(mol, implicitOnly=False, updateExplicitCount=False, sanitize=True) return mol else: rdmolops.Cleanup(mol) rdmolops.SanitizeMol(mol) mol = rdmolops.RemoveHs(mol, implicitOnly=False, updateExplicitCount=False, sanitize=True) return mol def _rare_filters(mol): if mol: cyano_filter = &quot;[C-]#[N+]&quot; oh_filter = &quot;[OH+]&quot; sulfur_filter = &quot;[SH]&quot; sulfur_filter2 = &quot;[SH2]&quot; iodine_filter1 = &quot;[IH2]&quot; iodine_filter2 = &quot;[I-]&quot; iodine_filter3 = &quot;[I+]&quot; if not mol.HasSubstructMatch(Chem.MolFromSmarts(cyano_filter)) and not mol.HasSubstructMatch(Chem.MolFromSmarts(oh_filter)) and not mol.HasSubstructMatch(Chem.MolFromSmarts(sulfur_filter2)) and not mol.HasSubstructMatch(Chem.MolFromSmarts(iodine_filter1)) and not mol.HasSubstructMatch(Chem.MolFromSmarts(iodine_filter2)) and not mol.HasSubstructMatch(Chem.MolFromSmarts(iodine_filter3)) and not mol.HasSubstructMatch(Chem.MolFromSmarts(sulfur_filter)): return mol def set_active(row): active = &#39;Inactive&#39; if row[&#39;standard_value&#39;] &lt;= 10000: active = &#39;Active&#39; return active . . class SanitizeDataset(): def __init__(self,df,smiles_column:str=&#39;Smiles&#39;,activity_column = None, sep:str=&#39; t&#39;): self.df, self.smiles_column, self.activity_column = df, smiles_column, activity_column @patch @delegates(SanitizeDataset) def add_mol_column(x:SanitizeDataset, smiles, **kwargs): if type(smiles) == str and smiles != &#39;&#39;: mol = Chem.MolFromSmiles(smiles) #.replace(&#39;@&#39;,&#39;&#39;).replace(&#39;/&#39;,&#39;&#39;).replace(&quot; &quot;,&quot;&quot;) return mol else: return None @patch @delegates(SanitizeDataset) def remove_unwanted(x:SanitizeDataset,mol, **kwargs): if not mol.HasSubstructMatch(_unwanted) and (sum([atom.GetIsotope() for atom in mol.GetAtoms()])==0): return mol @patch @delegates(SanitizeDataset) def _remove_salts(x:SanitizeDataset,mol, **kwargs): mol = _saltremover.StripMol(mol, dontRemoveEverything=True) mol = _saltremover.StripMol(mol, dontRemoveEverything=True) mol = rdmolops.RemoveHs(mol, implicitOnly=False, updateExplicitCount=False, sanitize=True) mol = x._getlargestFragment(mol) return mol @patch @delegates(SanitizeDataset) def _getlargestFragment(x:SanitizeDataset, mol, **kwargs): frags = rdmolops.GetMolFrags(mol, asMols=True, sanitizeFrags=True) maxmol = None for mol in frags: if mol is None: continue if maxmol is None: maxmol = mol if maxmol.GetNumHeavyAtoms() &lt; mol.GetNumHeavyAtoms(): maxmol = mol return maxmol @patch @delegates(SanitizeDataset) def is_valid(x:SanitizeDataset, mol, min_heavy_atoms = 1, max_heavy_atoms = 9999, **kwargs): if mol: return _rare_filters(mol) if min_heavy_atoms &lt; mol.GetNumHeavyAtoms() &lt; max_heavy_atoms else None return None @patch @delegates(SanitizeDataset) def canonilize(x:SanitizeDataset, mol, **kwargs): if mol: return Chem.MolToSmiles(mol, isomericSmiles = True) if mol.GetNumHeavyAtoms()&gt;0 else print(type(mol)) else: return None @patch @delegates(SanitizeDataset) def process_data(x:SanitizeDataset, **kwargs): x.df = x.df.loc[~x.df[x.smiles_column].isnull()] x.df[&#39;mol&#39;] = x.df[x.smiles_column].apply(x.add_mol_column) x.df = x.df.loc[~x.df[&#39;mol&#39;].isnull()] x.df[&#39;mol&#39;] = x.df[&#39;mol&#39;].apply(x.remove_unwanted) x.df[&#39;mol&#39;] = x.df[&#39;mol&#39;].apply(_rare_filters) x.df = x.df.loc[~x.df[&#39;mol&#39;].isnull()] x.df[&#39;mol&#39;] = x.df[&#39;mol&#39;].apply(x._remove_salts) x.df[&#39;mol&#39;] = x.df[&#39;mol&#39;].apply(neutralise) x.df[&#39;mol&#39;] = x.df[&#39;mol&#39;].apply(x.is_valid) x.df = x.df.loc[~x.df[&#39;mol&#39;].isnull()] x.df[&#39;processed_smiles&#39;] = x.df[&#39;mol&#39;].apply(lambda x : Chem.MolToSmiles(x)) x.df = x.df.loc[~x.df[&#39;processed_smiles&#39;].isnull()] x.df.drop_duplicates(subset=&#39;processed_smiles&#39;,inplace=True) return x.df . . #sanitizer = SanitizeDataset(df=df, smiles_column=&#39;SMILES&#39;,activity_column = &#39;Solubility&#39;, sep=&#39;,&#39;) . .",
            "url": "https://marcossantanaioc.github.io/fiocruzcheminformatics/jupyter/2021/05/31/third.html",
            "relUrl": "/jupyter/2021/05/31/third.html",
            "date": " • May 31, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Using molecule images as input?",
            "content": "For some time now I&#39;ve been wondering how much chemistry is necessary to train a model for QSAR. Do we need hand-crafted descriptors, such as ECFP and MACCS fingerprints? Or maybe complex quantum mechanical properties calculated with state-of-the-art software? . On the last post, we used image-like input to train bioactivity classifier using fastai. But what if we tried with REAL images? . First, we will import some libraries, including rdkit to deal with molecules. . Import modules . from rdkit import Chem from collections import Counter from rdkit.Chem import Draw, AllChem from rdkit.Chem.Draw import rdMolDraw2D from rdkit.Chem import rdDepictor import os import numpy as np from PIL import Image, ImageOps from io import BytesIO from fastai.vision.all import * from sklearn.model_selection import train_test_split from tqdm.notebook import tqdm . . Load data . moldf = pd.read_csv(&#39;/home/marcossantana/Downloads/fxa_ic50_processed.csv&#39;,sep=&#39;;&#39;).reset_index(drop=True) . moldf[&#39;filename&#39;] = [f&#39;mols_imgs/mol_{i}.png&#39; for i in moldf.index] . moldf.head() . doc_id standard_value standard_type standard_relation pchembl molregno canonical_smiles chembl_id target_dictionary target_chembl_id l1 l2 l3 confidence_score act processed_smiles filename . 0 3476 | 44.4 | IC50 | = | 7.35 | 192068 | N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc3ccccc3n2)NS(=O)(=O)Cc2ccccc2)C1O | CHEMBL117716 | 194 | CHEMBL244 | Enzyme | Protease | Serine protease | 8 | Active | N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc3ccccc3n2)NS(=O)(=O)Cc2ccccc2)C1O | mols_imgs/mol_0.png | . 1 6512 | 180.0 | IC50 | = | 6.75 | 203908 | Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1-n1ccccc1=O | CHEMBL337921 | 194 | CHEMBL244 | Enzyme | Protease | Serine protease | 8 | Active | Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1-n1ccccc1=O | mols_imgs/mol_1.png | . 2 6512 | 120.0 | IC50 | = | 6.92 | 204329 | Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O | CHEMBL340500 | 194 | CHEMBL244 | Enzyme | Protease | Serine protease | 8 | Active | Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O | mols_imgs/mol_2.png | . 3 3476 | 311.0 | IC50 | = | 6.51 | 192044 | N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CNC(=O)c2cnccn2)NS(=O)(=O)Cc2ccccc2)C1O | CHEMBL117721 | 194 | CHEMBL244 | Enzyme | Protease | Serine protease | 8 | Active | N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CNC(=O)c2cnccn2)NS(=O)(=O)Cc2ccccc2)C1O | mols_imgs/mol_3.png | . 4 3476 | 6.1 | IC50 | = | 8.21 | 191486 | N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc(O)nc2)NS(=O)(=O)Cc2ccccc2)C1O | CHEMBL331807 | 194 | CHEMBL244 | Enzyme | Protease | Serine protease | 8 | Active | N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc(O)nc2)NS(=O)(=O)Cc2ccccc2)C1O | mols_imgs/mol_4.png | . . moldf.shape . (2129, 17) . . Note: Never forget to separate a validation set! . train,valid = train_test_split(moldf,stratify=moldf[&#39;act&#39;],test_size = 0.2) train_idx, valid_idx = train.index, valid.index del train, valid . Save images . Now we need a way to convert our molecules to images. We can use RDKit Draw class for that. I think the API is a quite complex, especially for non-coders. I&#39;ll not explain it in details here. Please, check the official RDKit documentation and this short tutorial about the new drawing code) . def generate_mol_images(mols): &#39;&#39;&#39;Generate images from a collection of molecules&#39;&#39;&#39; mols = list(map(Chem.MolFromSmiles, mols)) print(len(mols)) for i, mol in enumerate(tqdm(mols,total=len(mols))): mol2image(mol, filename=f&#39;mol_{i}.png&#39;) #imgs = torch.stack(list(map(mol2image, mols))) def mol2image(m,filename=&#39;&#39;,save_dir=&#39;mols_imgs&#39;): &#39;&#39;&#39;Draw RDKit molecules as images&#39;&#39;&#39; # print(os.path.join(save_dir,filename)) d2d = rdMolDraw2D.MolDraw2DCairo(400, 400) # or MolDraw2DSVG to get SVGs d2d.drawOptions().bondLineWidth=5 # bondLineWidth (if positive, this overrides the default line width for bonds) d2d.drawOptions().padding=0.05 d2d.DrawMolecule(m) d2d.FinishDrawing() if os.path.exists(save_dir): path = os.path.join(save_dir, filename) d2d.WriteDrawingText(path) else: os.mkdir(save_dir) path = os.path.join(save_dir, filename) d2d.WriteDrawingText(path) . The mol2image function receives as input a molecule and converts it to an image. Briefly, we set a canvas of shape 400 x 400 pixels and the define some drawing options using the drawOptions method; in this example we&#39;ll make the bonds larger (bondLineWidth = 5) and add a little bit of padding (padding = 0.05) to the canvas. This will give us a good baseline to easily draw molecules. You can play around with the drawing options if you wish. . After drawing, we save the images to a folder. Take a look at a sample: . . Now let&#39;s save our images! . generate_mol_images(moldf[&#39;processed_smiles&#39;].values) . 2129 . The ls method allows us to see the saved image files. . path = Path(&#39;mols_imgs&#39;) path.ls() . (#2129) [Path(&#39;mols_imgs/mol_538.png&#39;),Path(&#39;mols_imgs/mol_508.png&#39;),Path(&#39;mols_imgs/mol_1741.png&#39;),Path(&#39;mols_imgs/mol_628.png&#39;),Path(&#39;mols_imgs/mol_265.png&#39;),Path(&#39;mols_imgs/mol_1477.png&#39;),Path(&#39;mols_imgs/mol_90.png&#39;),Path(&#39;mols_imgs/mol_2056.png&#39;),Path(&#39;mols_imgs/mol_1046.png&#39;),Path(&#39;mols_imgs/mol_2099.png&#39;)...] . Datablock . In order to train our model, we first need to create a DataLoaders object containing our training and validation sets. First, we create our datablock, which is basically a blueprint that tells fastai how to get our dependent and independent variables, how to treat them (e.g. are they images, text or tabular? is the dependent variable categorical or continuous?) and some transformations, including resizing the images and applying augmentation methods to them. . d_block = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_x=ColReader(&#39;filename&#39;), get_y = ColReader(&#39;act&#39;), item_tfms=Resize(256), batch_tfms=Rotate(180),splitter=IndexSplitter(valid_idx)) . The DataBlock API is very handy! We can inspect if our blueprint is alright but using the summary method to try to create a batch of our data. If it fails, we can see in which step. It&#39;s a very nice debugging strategy. . d_block.summary(moldf) . Setting-up type transforms pipelines Collecting items from doc_id standard_value standard_type standard_relation pchembl 0 3476 44.4 IC50 = 7.35 1 6512 180.0 IC50 = 6.75 2 6512 120.0 IC50 = 6.92 3 3476 311.0 IC50 = 6.51 4 3476 6.1 IC50 = 8.21 ... ... ... ... ... ... 2124 109537 1023.0 IC50 = 5.99 2125 109537 3026.0 IC50 = 5.52 2126 109537 8451.0 IC50 = 5.07 2127 109537 5735.0 IC50 = 5.24 2128 109537 3.4 IC50 = 8.47 molregno 0 192068 1 203908 2 204329 3 192044 4 191486 ... ... 2124 2333070 2125 2319885 2126 2333406 2127 2325502 2128 709600 canonical_smiles 0 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc3ccccc3n2)NS(=O)(=O)Cc2ccccc2)C1O 1 Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1-n1ccccc1=O 2 Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O 3 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CNC(=O)c2cnccn2)NS(=O)(=O)Cc2ccccc2)C1O 4 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc(O)nc2)NS(=O)(=O)Cc2ccccc2)C1O ... ... 2124 CC1(C)OCC([C@]2(C)C=C3CC[C@@H]4C(C)(C)[C@H](O)CC[C@@]4(C)[C@@H]3CC2)O1 2125 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)([C@@H](O)CO)C=C3CC[C@@H]2C1(C)C 2126 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)(CO)C=C3CC[C@@H]2C1(C)C 2127 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)(O)CC=C3CC[C@@H]2C1(C)C 2128 CN1CCc2nc(C(=O)N[C@@H]3C[C@@H](C(=O)N(C)C)CC[C@@H]3NC(=O)C(=O)Nc3ccc(Cl)cn3)sc2C1 chembl_id target_dictionary target_chembl_id l1 l2 0 CHEMBL117716 194 CHEMBL244 Enzyme Protease 1 CHEMBL337921 194 CHEMBL244 Enzyme Protease 2 CHEMBL340500 194 CHEMBL244 Enzyme Protease 3 CHEMBL117721 194 CHEMBL244 Enzyme Protease 4 CHEMBL331807 194 CHEMBL244 Enzyme Protease ... ... ... ... ... ... 2124 CHEMBL4293622 194 CHEMBL244 Enzyme Protease 2125 CHEMBL4280434 194 CHEMBL244 Enzyme Protease 2126 CHEMBL4293958 194 CHEMBL244 Enzyme Protease 2127 CHEMBL4286054 194 CHEMBL244 Enzyme Protease 2128 CHEMBL1269025 194 CHEMBL244 Enzyme Protease l3 confidence_score act 0 Serine protease 8 Active 1 Serine protease 8 Active 2 Serine protease 8 Active 3 Serine protease 8 Active 4 Serine protease 8 Active ... ... ... ... 2124 Serine protease 9 Inactive 2125 Serine protease 9 Inactive 2126 Serine protease 9 Inactive 2127 Serine protease 9 Inactive 2128 Serine protease 9 Active processed_smiles 0 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc3ccccc3n2)NS(=O)(=O)Cc2ccccc2)C1O 1 Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1-n1ccccc1=O 2 Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O 3 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CNC(=O)c2cnccn2)NS(=O)(=O)Cc2ccccc2)C1O 4 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc(O)nc2)NS(=O)(=O)Cc2ccccc2)C1O ... ... 2124 CC1(C)OCC([C@]2(C)C=C3CC[C@@H]4C(C)(C)[C@H](O)CC[C@@]4(C)[C@@H]3CC2)O1 2125 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)([C@@H](O)CO)C=C3CC[C@@H]2C1(C)C 2126 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)(CO)C=C3CC[C@@H]2C1(C)C 2127 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)(O)CC=C3CC[C@@H]2C1(C)C 2128 CN1CCc2nc(C(=O)N[C@@H]3C[C@@H](C(=O)N(C)C)CC[C@@H]3NC(=O)C(=O)Nc3ccc(Cl)cn3)sc2C1 filename 0 mols_imgs/mol_0.png 1 mols_imgs/mol_1.png 2 mols_imgs/mol_2.png 3 mols_imgs/mol_3.png 4 mols_imgs/mol_4.png ... ... 2124 mols_imgs/mol_2124.png 2125 mols_imgs/mol_2125.png 2126 mols_imgs/mol_2126.png 2127 mols_imgs/mol_2127.png 2128 mols_imgs/mol_2128.png [2129 rows x 17 columns] Found 2129 items 2 datasets of sizes 1703,426 Setting up Pipeline: ColReader -- {&#39;cols&#39;: &#39;filename&#39;, &#39;pref&#39;: &#39;&#39;, &#39;suff&#39;: &#39;&#39;, &#39;label_delim&#39;: None} -&gt; PILBase.create Setting up Pipeline: ColReader -- {&#39;cols&#39;: &#39;act&#39;, &#39;pref&#39;: &#39;&#39;, &#39;suff&#39;: &#39;&#39;, &#39;label_delim&#39;: None} -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: ColReader -- {&#39;cols&#39;: &#39;filename&#39;, &#39;pref&#39;: &#39;&#39;, &#39;suff&#39;: &#39;&#39;, &#39;label_delim&#39;: None} -&gt; PILBase.create starting from doc_id 6512 standard_value 120 standard_type IC50 standard_relation = pchembl 6.92 molregno 204329 canonical_smiles Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O chembl_id CHEMBL340500 target_dictionary 194 target_chembl_id CHEMBL244 l1 Enzyme l2 Protease l3 Serine protease confidence_score 8 act Active processed_smiles Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O filename mols_imgs/mol_2.png Name: 2, dtype: object applying ColReader -- {&#39;cols&#39;: &#39;filename&#39;, &#39;pref&#39;: &#39;&#39;, &#39;suff&#39;: &#39;&#39;, &#39;label_delim&#39;: None} gives mols_imgs/mol_2.png applying PILBase.create gives PILImage mode=RGB size=400x400 Pipeline: ColReader -- {&#39;cols&#39;: &#39;act&#39;, &#39;pref&#39;: &#39;&#39;, &#39;suff&#39;: &#39;&#39;, &#39;label_delim&#39;: None} -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from doc_id 6512 standard_value 120 standard_type IC50 standard_relation = pchembl 6.92 molregno 204329 canonical_smiles Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O chembl_id CHEMBL340500 target_dictionary 194 target_chembl_id CHEMBL244 l1 Enzyme l2 Protease l3 Serine protease confidence_score 8 act Active processed_smiles Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O filename mols_imgs/mol_2.png Name: 2, dtype: object applying ColReader -- {&#39;cols&#39;: &#39;act&#39;, &#39;pref&#39;: &#39;&#39;, &#39;suff&#39;: &#39;&#39;, &#39;label_delim&#39;: None} gives Active applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(0) Final sample: (PILImage mode=RGB size=400x400, TensorCategory(0)) Collecting items from doc_id standard_value standard_type standard_relation pchembl 0 3476 44.4 IC50 = 7.35 1 6512 180.0 IC50 = 6.75 2 6512 120.0 IC50 = 6.92 3 3476 311.0 IC50 = 6.51 4 3476 6.1 IC50 = 8.21 ... ... ... ... ... ... 2124 109537 1023.0 IC50 = 5.99 2125 109537 3026.0 IC50 = 5.52 2126 109537 8451.0 IC50 = 5.07 2127 109537 5735.0 IC50 = 5.24 2128 109537 3.4 IC50 = 8.47 molregno 0 192068 1 203908 2 204329 3 192044 4 191486 ... ... 2124 2333070 2125 2319885 2126 2333406 2127 2325502 2128 709600 canonical_smiles 0 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc3ccccc3n2)NS(=O)(=O)Cc2ccccc2)C1O 1 Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1-n1ccccc1=O 2 Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O 3 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CNC(=O)c2cnccn2)NS(=O)(=O)Cc2ccccc2)C1O 4 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc(O)nc2)NS(=O)(=O)Cc2ccccc2)C1O ... ... 2124 CC1(C)OCC([C@]2(C)C=C3CC[C@@H]4C(C)(C)[C@H](O)CC[C@@]4(C)[C@@H]3CC2)O1 2125 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)([C@@H](O)CO)C=C3CC[C@@H]2C1(C)C 2126 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)(CO)C=C3CC[C@@H]2C1(C)C 2127 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)(O)CC=C3CC[C@@H]2C1(C)C 2128 CN1CCc2nc(C(=O)N[C@@H]3C[C@@H](C(=O)N(C)C)CC[C@@H]3NC(=O)C(=O)Nc3ccc(Cl)cn3)sc2C1 chembl_id target_dictionary target_chembl_id l1 l2 0 CHEMBL117716 194 CHEMBL244 Enzyme Protease 1 CHEMBL337921 194 CHEMBL244 Enzyme Protease 2 CHEMBL340500 194 CHEMBL244 Enzyme Protease 3 CHEMBL117721 194 CHEMBL244 Enzyme Protease 4 CHEMBL331807 194 CHEMBL244 Enzyme Protease ... ... ... ... ... ... 2124 CHEMBL4293622 194 CHEMBL244 Enzyme Protease 2125 CHEMBL4280434 194 CHEMBL244 Enzyme Protease 2126 CHEMBL4293958 194 CHEMBL244 Enzyme Protease 2127 CHEMBL4286054 194 CHEMBL244 Enzyme Protease 2128 CHEMBL1269025 194 CHEMBL244 Enzyme Protease l3 confidence_score act 0 Serine protease 8 Active 1 Serine protease 8 Active 2 Serine protease 8 Active 3 Serine protease 8 Active 4 Serine protease 8 Active ... ... ... ... 2124 Serine protease 9 Inactive 2125 Serine protease 9 Inactive 2126 Serine protease 9 Inactive 2127 Serine protease 9 Inactive 2128 Serine protease 9 Active processed_smiles 0 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc3ccccc3n2)NS(=O)(=O)Cc2ccccc2)C1O 1 Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1-n1ccccc1=O 2 Cc1cc(NC(=O)Cc2ccc3[nH]c(-c4ccc(Cl)s4)nc3c2)ccc1N1CCOCC1=O 3 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CNC(=O)c2cnccn2)NS(=O)(=O)Cc2ccccc2)C1O 4 N=C(N)N1CCC[C@H](NC(=O)CNC(=O)[C@@H](CCNC(=O)c2ccc(O)nc2)NS(=O)(=O)Cc2ccccc2)C1O ... ... 2124 CC1(C)OCC([C@]2(C)C=C3CC[C@@H]4C(C)(C)[C@H](O)CC[C@@]4(C)[C@@H]3CC2)O1 2125 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)([C@@H](O)CO)C=C3CC[C@@H]2C1(C)C 2126 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)(CO)C=C3CC[C@@H]2C1(C)C 2127 CC(=O)O[C@@H]1CC[C@@]2(C)[C@@H]3CC[C@](C)(O)CC=C3CC[C@@H]2C1(C)C 2128 CN1CCc2nc(C(=O)N[C@@H]3C[C@@H](C(=O)N(C)C)CC[C@@H]3NC(=O)C(=O)Nc3ccc(Cl)cn3)sc2C1 filename 0 mols_imgs/mol_0.png 1 mols_imgs/mol_1.png 2 mols_imgs/mol_2.png 3 mols_imgs/mol_3.png 4 mols_imgs/mol_4.png ... ... 2124 mols_imgs/mol_2124.png 2125 mols_imgs/mol_2125.png 2126 mols_imgs/mol_2126.png 2127 mols_imgs/mol_2127.png 2128 mols_imgs/mol_2128.png [2129 rows x 17 columns] Found 2129 items 2 datasets of sizes 1703,426 Setting up Pipeline: ColReader -- {&#39;cols&#39;: &#39;filename&#39;, &#39;pref&#39;: &#39;&#39;, &#39;suff&#39;: &#39;&#39;, &#39;label_delim&#39;: None} -&gt; PILBase.create Setting up Pipeline: ColReader -- {&#39;cols&#39;: &#39;act&#39;, &#39;pref&#39;: &#39;&#39;, &#39;suff&#39;: &#39;&#39;, &#39;label_delim&#39;: None} -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (256, 256), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Rotate -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 1.0} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (256, 256), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=400x400, TensorCategory(0)) applying Resize -- {&#39;size&#39;: (256, 256), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=256x256, TensorCategory(0)) applying ToTensor gives (TensorImage of size 3x256x256, TensorCategory(0)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Rotate -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 1.0} starting from (TensorImage of size 4x3x256x256, TensorCategory([0, 0, 0, 0], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x256x256, TensorCategory([0, 0, 0, 0], device=&#39;cuda:0&#39;)) applying Rotate -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x256x256, TensorCategory([0, 0, 0, 0], device=&#39;cuda:0&#39;)) . . No errors? Nice! . Now we can create our dataloaders. . dls = d_block.dataloaders(moldf,bs=32) . dls.show_batch(max_n=4) . Train model . learn = cnn_learner(dls, resnet34, metrics=MatthewsCorrCoef()) . learn.fine_tune(10,base_lr = 5e-4, freeze_epochs=5) . epoch train_loss valid_loss matthews_corrcoef time . 0 | 1.279663 | 0.927331 | 0.156844 | 00:20 | . 1 | 1.133440 | 0.756082 | 0.219890 | 00:19 | . 2 | 1.048430 | 0.702097 | 0.249759 | 00:19 | . 3 | 0.950931 | 0.667921 | 0.327793 | 00:19 | . 4 | 0.885842 | 0.627575 | 0.313518 | 00:18 | . epoch train_loss valid_loss matthews_corrcoef time . 0 | 0.798137 | 0.593731 | 0.372278 | 00:26 | . 1 | 0.741893 | 0.603974 | 0.332780 | 00:26 | . 2 | 0.706047 | 0.553811 | 0.410596 | 00:26 | . 3 | 0.661693 | 0.566272 | 0.448788 | 00:26 | . 4 | 0.626662 | 0.556986 | 0.485671 | 00:26 | . 5 | 0.585711 | 0.572504 | 0.400680 | 00:26 | . 6 | 0.559231 | 0.550696 | 0.504990 | 00:25 | . 7 | 0.548869 | 0.506300 | 0.508802 | 00:26 | . 8 | 0.508134 | 0.506491 | 0.496422 | 00:25 | . 9 | 0.481029 | 0.506984 | 0.485368 | 00:25 | . . The matthew&#39;s correlation coefficient is ~0.50. Not bad at all! . learn.save(&#39;fit1&#39;) . Path(&#39;models/fit1.pth&#39;) . learn.export() . Interpretation . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix() . As we can see, it is possible to train a model using only images and get a decent MCC. We didn&#39;t even optimize the model or try different data augmentation methods. . I wonder how the model is actually making a prediction. Is it focusing on specific chemical groups? Can we apply interpretation tools used in computer vision? Those are interesting questions that we might take a look in future posts! . Do you have any ideas about possible improvements? Please, let me know! . Fin .",
            "url": "https://marcossantanaioc.github.io/fiocruzcheminformatics/jupyter/2020/12/14/second.html",
            "relUrl": "/jupyter/2020/12/14/second.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "How much chemistry to be useful?",
            "content": "from rdkit.Chem import MolFromSmiles, MolToSmiles from fastai.vision.all import * from rdkit.Chem import AllChem from rdkit import Chem import numpy as np import pandas as pd import IPython.display from PIL import Image . . Hi! This is will be the first post from my new blog. I hope you guys enjoy it. . I&#39;m doing the Fast.AI course and I decided to try solve some problems from my field of research (Cheminformatics) using the fastai package. In this blog, I will try to follow the chapters from the Deep Learning for Coders with fastai &amp; Pytorch. For instance, this notebook was inspired by chapter 2 (the notebook version can be found here) . Let&#39;s begin! . So, what are we going to do? . In this notebook we will try to solve a quantitative structure-activity relationship QSAR problem. What does this mean? Well, QSAR is a major aspect of Cheminformatics. The goal of QSAR is to find useful information in chemical and biological data and use this information for inference on new data. For example, we might want to predict if a molecule is active on a particular biological target. We could start with a dataset of experimentally measured bioactivities (e.g. $IC_{50}$ values, inhibition constants etc) and train a model for bioactivity prediction. This model can be used to predict the bioactivity of other molecules of interest. By using QSAR, BigPharma and research groups can generate new hypothesis much faster and cheaper than testing a bunch of molecules in vitro. . Traditionally, machine learning methods such as random forest, support vector machines and gradient boosting dominate the field. That&#39;s because these classical methods usually give very good results for a range of datasets and are quite easy to train. Until recently, researchers did not apply deep learning in large scale to bioactivity prediction. When they did, it was usually in the form of fully connected neural with just 2-5 layers. However, the last 5 years saw a BOOM in the number of publications using deep learning in very interesting ways! For example, recurrent neural networks are being applied to generate molecules, convnets are showing SOTA performance on binding affinity and pose prediction and multi-task learning was used successfully to win a Kaggle competition for bioactivity prediction! . The most common type of data for QSAR is tabular. Researchers usually calculate many chemical features to describe a collection of molecules. As an example, one of the most common consists in a binary vector indicating the presence/absence of chemical groups in a molecule. We can then use this fingerprint to train a macihine learning model for bioactivity prediction. . In this notebook we will use a different strategy. Instead of calculating a bunch of vectors, we&#39;ll convert each molecule to an image and feed that input to a neural network! As Jeremy said in the book: . Another point to consider is that although your problem might not look like a computer vision problem, it might be possible with a little imagination to turn it into one. For instance, if what you are trying to classify are sounds, you might try converting the sounds into images of their acoustic waveforms and then training a model on those images. . Let&#39;s try that! . How can we convert molecules to images? . In reality, we are not going to convert molecules to images of molecules. What we actually need is a way to represent molecules the same way as images. That way consists of using arrays. For example, an image can be represented as 3D array of shape $(W, H, C)$, where $W$ is the width, $H$ is the height and $C$ is the number of channels. If we could do that to molecules, then it would be straightforward to use it as input to a model. . There are many ways to do that, but we are going to use one that I think is very interesting. In 2017, Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas and Nathan Baker published a preprint showing that machine learning models actually don&#39;t need to know much about chemistry or biology to make a prediction! . In their original manuscript, the authors called their model Chemception and showed that using very, very simple image-like inputs it was possible to achieve SOTA performance on some public datasets. That&#39;s quite an achievement! Until yesterday, the cheminformatics community was using handcrafted features and now it seems we don&#39;t even need to tell many things about molecules to train a predictive model! . As the author mentioned in the preprint: . In addition, it should be emphasized that no additional source of chemistry-inspired features or inputs, such as molecular descriptors or fingerprints were used in training the model. This means that Chemception was not explicitly provided with even the most basic chemical concepts like “valency” or “periodicity”. . This means that Chemception had to learn A LOT about chemistry from scratch, using only not very informative inputs (to humans, at least)! . I really find this amazing! . Just to clarify:I&#39;m not saying the model would be useful in real settings. But it is quite amazing to see a good performance without using elaborate chemical descriptors. . The Chemception model is a convolutional neural network for QSAR tasks. An overview of their method is shown below: . . Load data . mols = pd.read_csv(&#39;/home/marcossantana/Documentos/GitHub/fiocruzcheminformatics/_data/fxa_ic50_processed.csv&#39;,sep=&#39;;&#39;) . mols.head(2) . doc_id standard_value standard_type standard_relation pchembl molregno canonical_smiles chembl_id target_dictionary target_chembl_id l1 l2 l3 confidence_score act processed_smiles is_valid . 0 47181 | 1.5 | IC50 | = | 8.82 | 459679 | COc1ccc(NC(=O)c2ccc(C(=N)N(C)C)cc2)c(C(=O)Nc2ccc(Cl)cn2)c1 | CHEMBL512351 | 194 | CHEMBL244 | Enzyme | Protease | Serine protease | 8 | Active | COc1ccc(NC(=O)c2ccc(C(=N)N(C)C)cc2)c(C(=O)Nc2ccc(Cl)cn2)c1 | False | . 1 30088 | 29000.0 | IC50 | = | 4.54 | 655811 | Cc1ccc(Oc2nc(Oc3cccc(C(=N)N)c3)c(F)c(NC(C)CCc3ccccc3)c2F)c(C(=O)O)c1 | CHEMBL193933 | 194 | CHEMBL244 | Enzyme | Protease | Serine protease | 9 | Inactive | Cc1ccc(Oc2nc(Oc3cccc(C(=N)N)c3)c(F)c(NC(C)CCc3ccccc3)c2F)c(C(=O)O)c1 | False | . The chemcepterize_mol function below will take care of converting the molecules SMILES strings (a one-line representation of the chemical structure) to the image-like format that we want. . Our workflow will go like this: first we define an embedding dimension and a resolution. You can think of this is a black canvas where the structures will be plotted and each atom will have a resolution consisting of how many pixels will be used to represent it. The dimensions of the canvas will be given by: . $$DIM = frac{EMBED*2}{RES}$$ . where $EMBED$ is the embedding size and $RES$ the resolution . The next step consists of calculating some basic chemical information from the structure, such as bond order, charges, atomic numbers and the hybridization states. This information will be converted into a matrix of shape $(P, DIM, DIM)$, where $P$ is the number of properties or channels in the image (in this case we will use 3) and $DIM$ is the dimension of the canvas. . The properties we will calculated are shown in the figure below from the original manuscript. . . def chemcepterize_mol(mol, embed=20.0, res=0.5): dims = int(embed*2/res) #print(dims) #print(mol) #print(&quot;,,,,,,,,,,,,,,,,,,,,,,&quot;) cmol = Chem.Mol(mol.ToBinary()) #print(cmol) #print(&quot;,,,,,,,,,,,,,,,,,,,,,,&quot;) cmol.ComputeGasteigerCharges() AllChem.Compute2DCoords(cmol) coords = cmol.GetConformer(0).GetPositions() #print(coords) #print(&quot;,,,,,,,,,,,,,,,,,,,,,,&quot;) vect = np.zeros((dims+2,dims+2,4)) # I added 2 pixels on to height and width because this function sometimes does not work if the molecule is too big. #Bonds first for i,bond in enumerate(mol.GetBonds()): bondorder = bond.GetBondTypeAsDouble() bidx = bond.GetBeginAtomIdx() eidx = bond.GetEndAtomIdx() bcoords = coords[bidx] ecoords = coords[eidx] frac = np.linspace(0,1,int(1/res*2)) # for f in frac: c = (f*bcoords + (1-f)*ecoords) idx = int(round((c[0] + embed)/res)) idy = int(round((c[1]+ embed)/res)) #Save in the vector first channel vect[ idx , idy ,0] = bondorder #Atom Layers for i,atom in enumerate(cmol.GetAtoms()): idx = int(round((coords[i][0] + embed)/res)) idy = int(round((coords[i][1]+ embed)/res)) #Atomic number vect[ idx , idy, 1] = atom.GetAtomicNum() #Hybridization hyptype = atom.GetHybridization().real vect[ idx , idy, 2] = hyptype #Gasteiger Charges charge = atom.GetProp(&quot;_GasteigerCharge&quot;) vect[ idx , idy, 3] = charge return Tensor(vect[:, :, :3].T) # We will omit the last dimension just to fit our fastai models. But you can also adapt the architeture to deal with 4 or more channels. . . . Note: an embedding of 32 will give a 128 x 128 canvas. We can use this correlation to generate images of any size. (224 * 32)/128. . First, we will create a column called mol that maps our molecular structures to rdkit.Chem.rdchem.Mol objects. This is essential because we are going to use Rdkit to calculate everything and rdkit.Chem.rdchem.Mol has a bunch of nice functionalities to work with molecular graphs. . mol= MolFromSmiles(&#39;c1ccccc1&#39;) # A rdkit.Chem.rdchem.Mol object representing benzene type(mol) . rdkit.Chem.rdchem.Mol . mols[&#39;mol&#39;] = mols[&#39;processed_smiles&#39;].apply(MolFromSmiles) . Now we are going to vectorize our molecules and transform them to image-like matrices.But first, let&#39;s test our function. . def vectorize(mol, embed, res): return chemcepterize_mol(mol, embed=embed, res=res) . v = vectorize(mols[&quot;mol&quot;][0],embed=56, res=0.5).T.numpy() . Ok, now let&#39;s see what that does! . As you can see, the image is mostly black space and the molecule is just a tiny, tiny part of it (shown in red). The black spaces have no chemical information at all! That&#39;s why the authors said Chemception had to learn everything from scratch! . plt.show(print(v.shape)) plt.imshow(v) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . (226, 226, 3) . &lt;matplotlib.image.AxesImage at 0x7fac6c69b590&gt; . We can make bigger molecules by reducing the embedding size. But beware that will also reduce the total image size. . larger_img = vectorize(mols[&quot;mol&quot;][0],embed=16, res=0.5).T.numpy() . plt.show(print((larger_img.shape))) plt.imshow(larger_img) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . (66, 66, 3) . &lt;matplotlib.image.AxesImage at 0x7fac6c601150&gt; . mols[&quot;molimage&quot;] = mols[&quot;mol&quot;].apply(partial(vectorize, embed=32, res=0.25)) . plt.imshow(mols[&quot;molimage&quot;][0].T.numpy()) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . &lt;matplotlib.image.AxesImage at 0x7fac6c51ce90&gt; . Training our model . Now that we know how to convert the molecules to the desired format, we are ready to train a model using fastai! In order to do that, we need to define three things . 1) How to split data . First, let&#39;s define how to split our data into training and validation sets. Luckly, our dataset comes with a column called &quot;is_valid&quot; showing which molecules should be used for validation and training. Therefore, we will use the ColSplitter class from fastai to get the indeces. We could also do a random split here or any kind at all. Fastai is very flexible! . splits = ColSplitter(&#39;is_valid&#39;)(mols) splits . ((#1703) [0,1,2,3,4,5,6,7,8,9...], (#426) [1703,1704,1705,1706,1707,1708,1709,1710,1711,1712...]) . 2) Define how to get the items . We need to tell fastai how to get the items that we&#39;ll feed to our model. In this case, we will use the images we created and stored in the &quot;molimage&quot; column of our dataframe. . x_tfms = ColReader(&#39;molimage&#39;) . 3) Define the targets . Now we need to tell fastai where are our targets. In this case, our targets are in the column &quot;act&quot;, showing the bioactivity of each molecule. We will also tell fastai to treat the values of this column as categories, which will be used to train a classification model. . y_tfms = [ColReader(&#39;act&#39;),Categorize] . The fastai book uses the DataBlock functionality to create the dataloaders at Chapter 2 and Jeremy says that we actually need four things . What kind of data to work with; | How to get the items; | How to label these items; | How to create a validation set. | But since we are using a custom data type, we&#39;ll skip the step defining the kind of data. . Create dataset . Now we can create our dataset: . mol_dataset = Datasets(mols,[x_tfms,y_tfms], splits=splits) . x,y = mol_dataset[0] x.shape,y . . (torch.Size([3, 258, 258]), TensorCategory(0)) . Create dataloaders . dls = mol_dataset.dataloaders(batch_size=8) . Let&#39;s inspect one batch to see if everything is already: . x,y = dls.one_batch() . x,y . (tensor([[[[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]], [[[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]], [[[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]], ..., [[[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]], [[[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]], [[[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]]], device=&#39;cuda:0&#39;), TensorCategory([0, 0, 1, 1, 0, 1, 1, 0], device=&#39;cuda:0&#39;)) . . It seems everything is in order. Alright! Let&#39;s train this beast! . Fit . metrics = [Recall(pos_label=1),Recall(pos_label = 0), Precision(pos_label = 0), MatthewsCorrCoef()] . learn = cnn_learner(dls, resnet18, metrics=metrics,ps=0.25) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /home/marcossantana/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . . Note: We are going to use Restnet18 instead of the original Inception architeture. . learn.fine_tune(10) . epoch train_loss valid_loss recall_score recall_score precision_score matthews_corrcoef time . 0 | 1.108745 | 1.240608 | 0.331169 | 0.716912 | 0.654362 | 0.050385 | 00:15 | . epoch train_loss valid_loss recall_score recall_score precision_score matthews_corrcoef time . 0 | 0.754158 | 0.685205 | 0.292208 | 0.897059 | 0.691218 | 0.241307 | 00:18 | . 1 | 0.698066 | 0.897160 | 0.201299 | 0.786765 | 0.635015 | -0.014106 | 00:18 | . 2 | 0.630345 | 0.794150 | 0.396104 | 0.750000 | 0.686869 | 0.152768 | 00:18 | . 3 | 0.607187 | 0.601677 | 0.512987 | 0.794118 | 0.742268 | 0.317116 | 00:18 | . 4 | 0.556304 | 0.543766 | 0.538961 | 0.830882 | 0.760943 | 0.386714 | 00:19 | . 5 | 0.392019 | 0.590917 | 0.558442 | 0.849265 | 0.772575 | 0.428208 | 00:18 | . 6 | 0.336817 | 0.561924 | 0.487013 | 0.882353 | 0.752351 | 0.409180 | 00:19 | . 7 | 0.271662 | 0.635271 | 0.461039 | 0.882353 | 0.743034 | 0.385314 | 00:19 | . 8 | 0.192005 | 0.622967 | 0.577922 | 0.834559 | 0.777397 | 0.426781 | 00:19 | . 9 | 0.189609 | 0.632492 | 0.636364 | 0.819853 | 0.799283 | 0.461058 | 00:18 | . It seems everything went pretty well! In addition, the Matthew&#39;s correlation coefficient is quite decent (~0.46). . Interpretation . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix() . The performance is pretty decent, if without any kind of data augmentation and hyperparameter optimization. . Let&#39;s try with a little bit of data augmentation. . In the original paper, the authors used random rotations of the images. Why is that? Well, since most of the image is empty space, if we distort it even a little bit, by cropping or squishing, it might completly change the molecule represented. Rotating the image is a solution to the data augmentation problem because in this particular case it won&#39;t change the meaning of our images. . dls = mol_dataset.dataloaders(batch_size=8,after_batch=Rotate(max_deg=180)) . learn = cnn_learner(dls, resnet18, metrics=metrics,ps=0.25) . learn.fine_tune(10) . epoch train_loss valid_loss recall_score recall_score precision_score matthews_corrcoef time . 0 | 1.085553 | 0.814067 | 0.428571 | 0.746324 | 0.697595 | 0.180596 | 00:13 | . epoch train_loss valid_loss recall_score recall_score precision_score matthews_corrcoef time . 0 | 0.740874 | 0.696819 | 0.500000 | 0.742647 | 0.724014 | 0.245222 | 00:18 | . 1 | 0.703036 | 0.758111 | 0.461039 | 0.720588 | 0.702509 | 0.183554 | 00:19 | . 2 | 0.653901 | 0.586844 | 0.383117 | 0.867647 | 0.712991 | 0.289424 | 00:18 | . 3 | 0.637761 | 0.676066 | 0.272727 | 0.794118 | 0.658537 | 0.076307 | 00:19 | . 4 | 0.537117 | 0.690890 | 0.435065 | 0.768382 | 0.706081 | 0.212265 | 00:18 | . 5 | 0.483605 | 0.649515 | 0.448052 | 0.860294 | 0.733542 | 0.341583 | 00:19 | . 6 | 0.348445 | 0.609656 | 0.500000 | 0.867647 | 0.753994 | 0.400096 | 00:19 | . 7 | 0.269874 | 0.642070 | 0.571429 | 0.827206 | 0.773196 | 0.411629 | 00:18 | . 8 | 0.203648 | 0.646935 | 0.506494 | 0.871324 | 0.757188 | 0.411164 | 00:19 | . 9 | 0.198849 | 0.674963 | 0.506494 | 0.871324 | 0.757188 | 0.411164 | 00:18 | . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix() . Exporting the model . Now that we trained the model, we can export it and use it for inference. . learn.export() . learn_inf = load_learner(&#39;export.pkl&#39;) . learn_inf.predict(mols[&#39;molimage&#39;][2]) . (&#39;Inactive&#39;, tensor(1), tensor([0.0191, 0.9809])) . Fin .",
            "url": "https://marcossantanaioc.github.io/fiocruzcheminformatics/jupyter/2020/12/04/first.html",
            "relUrl": "/jupyter/2020/12/04/first.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://marcossantanaioc.github.io/fiocruzcheminformatics/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m computational chemist working as a postdoc researcher at Fundação Oswaldo Cruz (FIOCRUZ), Brazil. This is my blog to post about cheminformatics themes. .",
          "url": "https://marcossantanaioc.github.io/fiocruzcheminformatics/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://marcossantanaioc.github.io/fiocruzcheminformatics/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}